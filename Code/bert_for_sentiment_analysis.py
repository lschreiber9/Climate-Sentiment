# -*- coding: utf-8 -*-
"""BERT for Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/166Vig1B8iE0YL-pt2aM_Hhj4M6cso1qQ

**Climate Sentiment Analysis** with **BERT**
This project uses a fine-tuned BERT model to analyze text from **ESG-Reports** of the **industrial companies** in the S&P 500. It classifies sentiment into Risk, Opportunity, or Neutral, providing insights into how organizations communicate about climate topics.

Classification Criteria

1. A text is classified as **"Risk"** if it focuses on:

*   Business downside risks, potential losses, or adverse developments.
*   Negative impacts of activities on society or the environment.
*   Specific negative descriptors of past, present, or anticipated developments.



2. A text is classified as **"Opportunity"** if it emphasizes:

* Business opportunities from climate change mitigation or adaptation.
* Positive impacts of activities on society or the environment.
* Specific positive descriptors of past, present, or anticipated developments.

3. Neutral
A text is classified as **"Neutral"** if it:

* States facts or developments without positive or negative framing.
* Lacks specific adjectives that indicate risk or opportunity.

# Training BERT
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Necessary Libraries"""

import pandas as pd
import torch
import gc

"""### Import Datasets from Huggingface

These Datasets are expert annotated.
"""

datasets = {
    "climate_specificity": {
        "train": "hf://datasets/climatebert/climate_specificity/data/train-00000-of-00001-298fad749f8929f7.parquet",
        "test": "hf://datasets/climatebert/climate_specificity/data/test-00000-of-00001-2588e03729a1bfe7.parquet"
    },
    "climate_sentiment": {
        "train": "hf://datasets/climatebert/climate_sentiment/data/train-00000-of-00001-04b49ae22f595095.parquet",
        "test": "hf://datasets/climatebert/climate_sentiment/data/test-00000-of-00001-3f9f7af4f5914b8e.parquet"
    },
    "climate_commitments_actions": {
        "train": "hf://datasets/climatebert/climate_commitments_actions/data/train-00000-of-00001-2044cce9e261c6b3.parquet",
        "test": "hf://datasets/climatebert/climate_commitments_actions/data/test-00000-of-00001-77f76c0960abb9c6.parquet"
    },
    "environmental_claims": {
        "train": "hf://datasets/climatebert/environmental_claims/data/train-00000-of-00001-98aa5228a06a17d0.parquet",
        "validation": "hf://datasets/climatebert/environmental_claims/data/validation-00000-of-00001-2553e47d408fab28.parquet",
        "test": "hf://datasets/climatebert/environmental_claims/data/test-00000-of-00001-79fd931297fff765.parquet"
    },
    "climate_detection": {
        "train": "hf://datasets/climatebert/climate_detection/data/train-00000-of-00001-4b831beb8839bf3e.parquet",
        "test": "hf://datasets/climatebert/climate_detection/data/test-00000-of-00001-87f8706e009e9b75.parquet"
    }
}

"""
### Load Dataset and Split Into Training and Testing"""

def load_data(dataset_name):
    if dataset_name not in datasets:
        raise ValueError(f"Dataset '{dataset_name}' is not defined.")

    dataset_info = datasets[dataset_name]
    train_df = pd.read_parquet(dataset_info["train"])
    test_df = pd.read_parquet(dataset_info["test"])

    return train_df, test_df

for dataset_name in datasets.keys():
    globals()[f"train_{dataset_name}"], globals()[f"test_{dataset_name}"] = load_data(dataset_name)

"""### Import Tokenizer"""

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

"""### Tokenize Text Data and Prepare Encodings and Labels"""

def preprocess_data(df, text_column, label_column, max_length=512):
    encodings = tokenizer(
        df[text_column].tolist(),
        truncation=True,
        padding=True,
        max_length=max_length,
        return_tensors="pt"
    )

    labels = df[label_column].tolist()

    return encodings, labels

"""### Preprocess Datasets in Training and Test Data"""

preprocessed_data = {}

for dataset_name in datasets.keys():
    train_df = globals()[f"train_{dataset_name}"]
    test_df = globals()[f"test_{dataset_name}"]

    train_encodings, train_labels = preprocess_data(train_df, text_column="text", label_column="label")
    test_encodings, test_labels = preprocess_data(test_df, text_column="text", label_column="label")

    preprocessed_data[dataset_name] = {
        "train": (train_encodings, train_labels),
        "test": (test_encodings, test_labels)
    }

"""### Encodings and Labels Wrapped Into a PyTorch Dataset

"""

from torch.utils.data import Dataset

class ClimateDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

"""### Efficient Batch Processing for Training and Testing

"""

from torch.utils.data import DataLoader

dataloaders = {}

for dataset_name, splits in preprocessed_data.items():
    train_encodings, train_labels = splits["train"]
    test_encodings, test_labels = splits["test"]

    train_dataset = ClimateDataset(train_encodings, train_labels)
    test_dataset = ClimateDataset(test_encodings, test_labels)

    dataloaders[dataset_name] = {
        "train": DataLoader(train_dataset, batch_size=16, shuffle=True),
        "test": DataLoader(test_dataset, batch_size=64, shuffle=False)
    }

"""### Choosing the BERT Model

"""

from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

"""### Define and Optimize Loss Function"""

from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)

"""### Choosing the Processing Unit

For the model training a 12 GB CPU was not sufficient. Hence, we used a GPU from the colab environment.
"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

"""### Training Loop for One Dataset"""

def train_model(model, optimizer, train_loader, test_loader, device, epochs=3):
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        correct = 0

        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            logits = outputs.logits

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            predictions = torch.argmax(logits, dim=1)
            correct += (predictions == labels).sum().item()

        accuracy = correct / len(train_loader.dataset)
        print(f"Epoch {epoch + 1}: Loss = {total_loss:.4f}, Accuracy = {accuracy:.4f}")

    evaluate_model(model, test_loader, device)

"""### Loss-Evaluation Function"""

def evaluate_model(model, test_loader, device):
    model.eval()
    correct = 0
    total_loss = 0

    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            logits = outputs.logits

            total_loss += loss.item()
            predictions = torch.argmax(logits, dim=1)
            correct += (predictions == labels).sum().item()

    accuracy = correct / len(test_loader.dataset)
    print(f"Test Loss = {total_loss:.4f}, Test Accuracy = {accuracy:.4f}")

"""### Fine Tuning BERT with Loaded Datasets

Initially, we were uncertain about the dataset selection. After careful consideration, we chose climate_sentiment to answer our research question. The code is flexible and can easily train the model on other datasets by changing the dataset_name.
"""

from datetime import datetime

dataset_name = "climate_sentiment"
if dataset_name in dataloaders:
    print(f"Training on {dataset_name} dataset...")

    loaders = dataloaders[dataset_name]
    train_loader = loaders["train"]
    test_loader = loaders["test"]

    num_labels = 3 if dataset_name == "climate_sentiment" else 2

    try:
        model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_labels)
        optimizer = AdamW(model.parameters(), lr=5e-5)
        model.to(device)

        print(f"Started training on {dataset_name} at {datetime.now()}")
        train_model(model, optimizer, train_loader, test_loader, device, epochs=3)

        save_path = f"/content/drive/MyDrive/Big Data Project/NLP analysis/{dataset_name}_model"
        model.save_pretrained(save_path)
        print(f"Model for {dataset_name} saved at {save_path}!")

    except RuntimeError as e:
        print(f"Error during training on {dataset_name}: {e}")
        torch.cuda.empty_cache()

    finally:
        del train_loader, test_loader
        del model, optimizer
        gc.collect()
        torch.cuda.empty_cache()

"""# Creating ESG-Reports Dataset

### Necessary Libraries
"""

!pip install PyMuPDF

import fitz
import os
import json
import re

"""### Extract Text from PDFs"""

pdf_directory = "/content/drive/MyDrive/Big Data Project/NLP analysis/text_files/Sustainability Report "
output_json_dir = "/content/drive/MyDrive/Big Data Project/NLP analysis/text_files/Extracted Text"

os.makedirs(output_json_dir, exist_ok=True)

def extract_text_as_json(pdf_path, output_file):
    pdf_data = {"file_name": os.path.basename(pdf_path), "pages": []}

    with fitz.open(pdf_path) as doc:
        for page_num, page in enumerate(doc, start=1):
            text = page.get_text()
            pdf_data["pages"].append({"page_number": page_num, "text": text})

    with open(output_file, "w") as outfile:
        json.dump(pdf_data, outfile, indent=4)

def process_pdfs_to_json(pdf_directory, output_json_dir):
    for filename in os.listdir(pdf_directory):
        if filename.endswith(".pdf"):
            pdf_path = os.path.join(pdf_directory, filename)
            output_file = os.path.join(output_json_dir, f"{os.path.splitext(filename)[0]}.json")

            if not os.path.exists(pdf_path):
                print(f"File not found: {pdf_path}. Skipping...")
                continue

            extract_text_as_json(pdf_path, output_file)
            print(f"Saved extracted text to {output_file}")

process_pdfs_to_json(pdf_directory, output_json_dir)

"""### Keep Only Climate Relevant Paragraphs

We use the list of climate related vocabulary from Binger et al. (2024). However, we leave out the following key words: "sustainable",
 "sustainability", "environmental" and "ESG". These terms are omitted as they appear on every page even when there is no content about climate related topics.






"""

text_folder = '/content/drive/MyDrive/Big Data Project/NLP analysis/text_files/Extracted Text'
output_folder = '/content/drive/MyDrive/Big Data Project/NLP analysis/text_files/climate_only_paragraphs'

os.makedirs(output_folder, exist_ok=True)

keywords = [
    "air quality", "bushfire", "carbon", "CH4", "climate", "climate-related", "CO2", "coal",
    "decarbonization", "decarbonisation", "deforestation", "drought", "emission",
    "energy consumption", "energy efficiency", "energy efficient", "energy transition",
    "footprint", "fossil", "GHG", "global warming", "greenhouse",
    "heat wave", "hurricane", "land use", "litigation risk", "low-carbon", "methane", "N2O",
    "natural hazard", "nitrous oxide", "O3", "ozone", "Paris Agreement", "physical risk",
    "renewable", "rural fire", "sea level", "social responsibility", "solar energy", "TCFD",
    "temperature rise", "transition risk", "tropical cyclone", "tropical storm", "typhoon",
    "weather", "wildfire", "wildland fire", "wind energy"
]

keywords_regex = re.compile(r'\b(?:' + '|'.join(re.escape(word) for word in keywords) + r')\b', re.IGNORECASE)

def filter_climate_segments(input_folder, output_folder):
    for file_name in filter(lambda f: f.endswith('.json'), os.listdir(input_folder)):
        input_path = os.path.join(input_folder, file_name)
        output_path = os.path.join(output_folder, file_name)

        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        text = data.get('text', '')
        filtered_segments = [
            segment.strip() for segment in text.split('\n\n') if keywords_regex.search(segment)
        ]
        data['text'] = '\n\n'.join(filtered_segments)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=4, ensure_ascii=False)

filter_climate_segments(text_folder, output_folder)

"""## Creating one dataset"""

text_folder = '/content/drive/MyDrive/Big Data Project/NLP analysis/text_files/climate_only_paragraphs'

combined_data = {}

for file_name in os.listdir(text_folder):
    if file_name.endswith('.json'):
        file_path = os.path.join(text_folder, file_name)
        with open(file_path, 'r', encoding='utf-8') as json_file:
            try:
                data = json.load(json_file)
                key = os.path.splitext(file_name)[0]
                combined_data[key] = data

output_path = '/content/drive/MyDrive/Big Data Project/NLP analysis/ESG-Report_dataset.json'
with open(output_path, 'w', encoding='utf-8') as json_output_file:
    json.dump(combined_data, json_output_file, indent=4, ensure_ascii=False)

"""# Sentiment Score from the climate_sentiment_model

### Necessary libraries
"""

import torch
import re
import os
import json
import csv
import pandas as pd

"""### Importing the climate_sentiment_model

The climate_sentiment_model classifies in three categories.

Risk: Business Risk due to climate change

Neutral: No business impact due to climate change

Opportunity: Business Opportunity due to climate change
"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Nutze GerÃ¤t: {device}")

text_folder = '/content/drive/MyDrive/Big Data Project/NLP analysis/ESG-Report_dataset.json'
base_model = "bert-base-uncased"
model_dir = "/content/drive/MyDrive/Big Data Project/NLP analysis/climate_models/climate_sentiment_model"

tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForSequenceClassification.from_pretrained(model_dir).to(device)

tokenizer.save_pretrained(model_dir)

"""### Split Dataset into Readable Chunks"""

def split_text_into_segments(text, max_length=512):

    paragraphs = re.split(r'\n\s*\n', text)

    segments = []
    current_segment = []
    current_length = 0

    for paragraph in paragraphs:
        tokens = tokenizer.tokenize(paragraph)
        token_length = len(tokens)

        if current_length + token_length > max_length:
            segments.append(" ".join(current_segment))
            current_segment = []
            current_length = 0

        current_segment.append(paragraph)
        current_length += token_length

    if current_segment:
        segments.append(" ".join(current_segment))

    return segments

"""### Functions for Label Prediction and Segment Analysis

Further we change the label risk from 1 to -1 to calculate the overall sentiment score for every ESG report.
"""

def predict_label(segment):
    inputs = tokenizer(segment, return_tensors="pt", truncation=True, padding=True, max_length=512)
    inputs = {key: value.to(device) for key, value in inputs.items()}
    outputs = model(**inputs)
    logits = outputs.logits
    probabilities = torch.softmax(logits, dim=1).detach().cpu().numpy()[0]

    predicted_class = int(torch.argmax(logits, dim=1).item())

    label_mapping = {0: 0, 1: -1, 2: 1}
    mapped_label = label_mapping[predicted_class]

    return mapped_label, probabilities

def analyze_file(file_input):
    if isinstance(file_input, dict):
        data = file_input
    else:
        with open(file_input, "r", encoding="utf-8") as file:
            data = json.load(file)

    text = data.get("content", "")

    segments = split_text_into_segments(text)

    segment_results = []
    for segment in segments:
        label, probabilities = predict_label(segment)
        segment_results.append({
            "segment": segment,
            "label": label,
            "probabilities": probabilities
        })

    overall_index = (sum(r["label"] for r in segment_results) / len(segment_results) if segment_results else 0) * 5
    return overall_index, segment_results

"""### Sentiment Analysis of Every ESG-Report"""

combined_json_path = '/content/drive/MyDrive/Big Data Project/NLP analysis/ESG-Report_dataset.json'

with open(combined_json_path, 'r') as f:
    combined_data = json.load(f)

results = {}
total_segments_analyzed = 0

# Iterate through each file-like entry in the combined dataset
for filename, file_content in combined_data.items():
    # Assuming `analyze_file` can handle the content directly as a dictionary
    overall_index, segment_results = analyze_file(file_content)  # Adjust this to match your `analyze_file` function
    results[filename] = {"overall_index": overall_index, "segment_results": segment_results}
    total_segments_analyzed += len(segment_results)

print(f"Total segments analyzed: {total_segments_analyzed}")

"""### Generate Random Sample for Manual Analysis"""

import random

num_examples = 10

segments = []
for file, result in results.items():
    for segment_result in result['segment_results']:
        segments.append({
            "file": file,
            "content": segment_result['segment'],
            "label": segment_result['label'],
            "probability": segment_result['probabilities']
        })

random_examples = random.sample(segments, min(num_examples, len(segments)))

for idx, example in enumerate(random_examples, 1):
    print(f"Example {idx}:")
    print(f"File: {example['file']}")
    print(f"Content: {example['content']}")
    print(f"Label: {example['label']}")
    print(f"Probability: {example['probability']}")
    print("-" * 40)

"""### Save Results for Further Analysis"""

from natsort import natsorted

output_csv = '/content/drive/MyDrive/Big Data Project/NLP analysis/sentiment_climate_only_text.csv'

sorted_results = natsorted(
    [(filename, data["overall_index"]) for filename, data in results.items()],
    key=lambda x: x[0]
)

df = pd.DataFrame(sorted_results, columns=["Filename", "Overall Index"])
df.to_csv(output_csv, index=False, encoding='utf-8')