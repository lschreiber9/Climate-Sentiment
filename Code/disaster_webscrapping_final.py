# -*- coding: utf-8 -*-
"""Disaster_Webscrapping_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11ueDflTSSEa_0gZro1G-55VAN_mhTa-0
"""

!pip install geopandas matplotlib
!pip install scrapy
!pip install basemap basemap-data-hires

"""##Final Code##

**Intermediary Steps: Scrapping an Actual Website**
"""

from bs4 import BeautifulSoup
import requests
import re
from collections import defaultdict

disaster_keywords = [
    "extreme heat", "heatwave", "heat dome", "urban heat island",
    "extreme cold", "polar vortex", "frost", "cold snap", "blizzard",
    "flood", "riverine flood", "urban flood", "coastal flood", "flash flood",
    "ice jam flood", "glacial lake outburst flood", "dam break flood", "mudslide",
    "wildfire", "forest fire", "bushfire", "peat fire", "urban wildfire", "wildfire smoke",
    "drought", "agricultural drought", "meteorological drought", "hydrological drought",
    "water scarcity", "desertification",
    "storm", "hurricane", "typhoon", "cyclone", "tornado", "hailstorm",
    "lightning storm", "derecho", "storm surge", "dust storm", "windstorm",
    "marine heatwave", "coral bleaching", "sea level rise", "coastal erosion",
    "permafrost thaw", "avalanche", "sea ice loss",
    "pest outbreaks", "disease outbreaks", "vector-borne diseases", "waterborne diseases",
    "fire-flood", "storm-fire", "drought-induced wildfire", "heatwave-power outage",
    "climate migration", "food insecurity", "economic losses"
]

states = [
    "Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado", "Connecticut", "Delaware", "Florida", "Georgia",
    "Hawaii", "Idaho", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", "Maine", "Maryland",
    "Massachusetts", "Michigan", "Minnesota", "Mississippi", "Missouri", "Montana", "Nebraska", "Nevada",
    "New Hampshire", "New Jersey", "New Mexico", "New York", "North Carolina", "North Dakota", "Ohio",
    "Oklahoma", "Oregon", "Pennsylvania", "Rhode Island", "South Carolina", "South Dakota", "Tennessee",
    "Texas", "Utah", "Vermont", "Virginia", "Washington", "West Virginia", "Wisconsin", "Wyoming"
]

url = "https://www.weather.gov/news/"

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

def get_relevant_headlines_by_state(debug=False):
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 403:
            print("Access denied: The server returned a 403 Forbidden error.")
            return defaultdict(list)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        titles = soup.find_all('a')

        if not titles:
            print("No headlines found. Check the webpage structure.")
            return defaultdict(list)

        keyword_pattern = re.compile(r'|'.join(disaster_keywords), re.IGNORECASE)
        state_pattern = re.compile(r'|'.join(states), re.IGNORECASE)
        year_pattern = re.compile(r"\b(2020|2021|2022|2023)\b")

        classified_headlines = defaultdict(list)
        for title in titles:
            headline = title.get_text(strip=True)
            if year_pattern.search(headline) and keyword_pattern.search(headline):
                matched_states = state_pattern.findall(headline)
                if matched_states:
                    for state in matched_states:
                        classified_headlines[state].append(headline)
                        if debug:
                            print(f"Matched headline: {headline} (State: {state})")

        return classified_headlines

    except requests.exceptions.HTTPError as http_err:
        print(f"HTTP error occurred: {http_err}")
    except Exception as err:
        print(f"An error occurred: {err}")

    return defaultdict(list)

classified_headlines = get_relevant_headlines_by_state(debug=False)

if classified_headlines:
    print("\nRelevant Headlines (2020-2023) by State:")
    for state, headlines in classified_headlines.items():
        print(f"\n{state}:")
        for i, headline in enumerate(headlines, start=1):
            print(f"  {i}. {headline}")
else:
    print("No relevant headlines found between 2020 and 2023.")

"""**Final scrapper incorporating scrapping Google news + Real Website**"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import json
import csv
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

disaster_keywords = [
    "extreme heat", "heatwave", "heat dome", "urban heat island",
    "extreme cold", "polar vortex", "frost", "cold snap", "blizzard",
    "flood", "riverine flood", "urban flood", "coastal flood", "flash flood",
    "ice jam flood", "glacial lake outburst flood", "dam break flood", "mudslide",
    "wildfire", "forest fire", "bushfire", "peat fire", "urban wildfire", "wildfire smoke",
    "drought", "agricultural drought", "meteorological drought", "hydrological drought",
    "water scarcity", "desertification",
    "storm", "hurricane", "typhoon", "cyclone", "tornado", "hailstorm",
    "lightning storm", "derecho", "storm surge", "dust storm", "windstorm",
    "marine heatwave", "coral bleaching", "sea level rise", "coastal erosion",
    "permafrost thaw", "avalanche", "sea ice loss",
    "pest outbreaks", "disease outbreaks", "vector-borne diseases", "waterborne diseases",
    "fire-flood", "storm-fire", "drought-induced wildfire", "heatwave-power outage",
    "climate migration", "food insecurity", "economic losses"
]
disaster_pattern = re.compile("|".join(disaster_keywords), re.IGNORECASE)

states = [
    "Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado", "Connecticut", "Delaware", "Florida", "Georgia",
    "Hawaii", "Idaho", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", "Maine", "Maryland",
    "Massachusetts", "Michigan", "Minnesota", "Mississippi", "Missouri", "Montana", "Nebraska", "Nevada",
    "New Hampshire", "New Jersey", "New Mexico", "New York", "North Carolina", "North Dakota", "Ohio",
    "Oklahoma", "Oregon", "Pennsylvania", "Rhode Island", "South Carolina", "South Dakota", "Tennessee",
    "Texas", "Utah", "Vermont", "Virginia", "Washington", "West Virginia", "Wisconsin", "Wyoming"
]

common_suffixes = [
    " Inc", " Corp", " plc", " Co", " Ltd", " LLC", " Group", " Holdings", " Intl",
    " Incorporated", " Company", " Limited", " Enterprises", " Technologies", " Solutions"
]

companies = {
    company: re.sub(f"({'|'.join(common_suffixes)})[.,]?$", "", company, flags=re.IGNORECASE).strip()
    for company in [
        "GE Aerospace", "Caterpillar Inc", "RTX Corporation", "Uber Technologies Inc.",
        "Union Pacific Corp", "Eaton Corp plc", "Honeywell Intl Inc", "Lockheed Martin",
        "Automatic Data Processing", "Deere & Co", "United Parcel Service Inc B", "Boeing Co",
        "Trane Technologies plc", "GE Vernova Inc", "Parker-Hannifin Corp", "General Dynamics",
        "TransDigm Group", "Waste Management Inc", "Northrop Grumman Corp", "Cintas Corp",
        "Illinois Tool Works Inc", "3M Co", "CSX Corporation", "Emerson Electric Co",
        "Carrier Global Corp.", "FedEx Corp", "Norfolk Southern Corp", "PACCAR Inc",
        "United Rentals Inc", "Johnson Controls International plc", "W.W. Grainger Inc",
        "L3Harris Technologies Inc", "Quanta Services Inc", "Copart Inc", "Cummins Inc",
        "Paychex Inc", "Fastenal Co", "Howmet Aerospace Inc.", "Republic Services Inc",
        "Otis Worldwide Corp", "Ingersoll Rand Inc.", "AMETEK Inc", "Verisk Analytics Inc",
        "Old Dominion Freight Line Inc", "Delta Air Lines", "Equifax Inc", "Wabtec",
        "Axon Enterprise Inc", "Xylem Inc", "Rockwell Automation Inc", "Fortive Corp",
        "Veralto Corp", "Dover Corp", "Broadridge Financial Solutions Inc.",
        "United Airlines Holding, Inc", "Hubbell Inc", "Leidos Holdings Inc", "Builders FirstSource",
        "Masco Corp", "Southwest Airlines Co", "Jacobs Solutions Inc.", "Snap On Inc",
        "Expeditors Intl of WA Inc", "Pentair PLC", "Stanley Black & Decker", "Textron Inc",
        "IDEX Corp", "J.B. Hunt Transport Services", "Nordson Corp", "Rollins Inc",
        "CH Robinson Worldwide Inc", "Allegion plc", "Dayforce, Inc.", "Huntington Ingalls Industries Inc.",
        "Generac Holdings Inc", "A.O. Smith Corp", "Paycom Software Inc", "Amentum Holdings Inc."
    ]
}

def scrape_google_news(query):
    logging.info(f"Scraping Google News for query: {query}")
    results = []
    rss_url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"

    try:
        response = requests.get(rss_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "xml")

        items = soup.find_all("item")
        seen_titles = set()

        for item in items:
            title = item.title.text.strip()
            description = item.description.text.strip() if item.description else "No description available"
            source = item.source.text.strip() if item.source else "Unknown"
            pub_date = item.pubDate.text if item.pubDate else None

            if title.lower() in seen_titles:
                continue
            seen_titles.add(title.lower())

            if pub_date:
                pub_date_obj = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %Z")
                if not (datetime(2020, 1, 1) <= pub_date_obj <= datetime(2023, 12, 31)):
                    continue
            else:
                continue

            if not (disaster_pattern.search(title) or disaster_pattern.search(description)):
                continue

            results.append({
                "title": title,
                "description": description,
                "source": source,
                "is_trusted": source.lower() in [
                    "bbc.com", "the washington post", "the guardian", "reuters",
                    "the new york times", "bloomberg", "the associated press", "cnn",
                    "al jazeera", "npr", "financial times", "the economist",
                    "nature.com", "national geographic", "scientific american",
                    "insideclimate news", "carbon brief", "climate central",
                    "south china morning post", "the straits times"
                ],
                "date": pub_date_obj.strftime("%Y-%m-%d")
            })

    except Exception as e:
        logging.error(f"Error scraping Google News for query '{query}': {e}")

    return results

def scrape_disaster_related_news():
    all_results = {}

    with ThreadPoolExecutor(max_workers=10) as executor:
        state_futures = {}
        company_futures = {}

        for company, cleaned_name in companies.items():
            query = f"{cleaned_name} climate disaster OR {cleaned_name} extreme weather OR {cleaned_name} climate impact"
            company_futures[executor.submit(scrape_google_news, query)] = company

        # State-level queries with delay for Florida and New York
        for state in states:
            query = f"{state} climate disaster OR {state} extreme weather OR {state} climate impact"

            if state in ["Florida", "New York"]:
                logging.info(f"Adding delay before scraping for {state}")
                time.sleep(5)

            state_futures[executor.submit(scrape_google_news, query)] = state

        for future in as_completed(state_futures):
            results = future.result()
            key = state_futures[future]

            if results:
                if key not in all_results:
                    all_results[key] = []
                all_results[key].extend(results)

        for future in as_completed(company_futures):
            results = future.result()
            key = company_futures[future]

            if results:
                if key not in all_results:
                    all_results[key] = []
                all_results[key].extend(results)

    # Identify entities with no climate news
    no_news_states = [state for state in states if state not in all_results]
    no_news_companies = [company for company in companies if company not in all_results]

    logging.info(f"States with no climate news: {', '.join(no_news_states)}")
    logging.info(f"Companies with no climate news: {', '.join(no_news_companies)}")

    print(f"\nMissing states with climate related news = {', '.join(no_news_states)}")

    # Print results to console
    for key, results in all_results.items():
        print(f"\n{'Company' if key in companies else 'State'}: {key}")
        for result in results:
            trusted_status = "Trusted Source" if result["is_trusted"] else "Untrusted Source"
            print(f"  - {result['title']} - {result['source']} ({trusted_status}, Published: {result['date']})")

    # Summary statistics
    company_count = sum(1 for key in all_results if key in companies)
    state_count = sum(1 for key in all_results if key in states)
    total_articles = sum(len(results) for results in all_results.values())

    articles_per_year = {}
    csv_rows = []
    for key, results in all_results.items():
        for result in results:
            year = result['date'][:4]
            articles_per_year[year] = articles_per_year.get(year, 0) + 1

            csv_rows.append({
                "Entity": key,
                "Type": "Company" if key in companies else "State",
                "Title": result['title'],
                "Source": result['source'],
                "Trusted": "Yes" if result['is_trusted'] else "No",
                "Date": result['date']
            })

    print("\nSummary Statistics:")
    print(f"Total Companies with Climate Related News: {company_count} out of {len(companies)}")
    print(f"Total States with Climate Related News: {state_count} out of {len(states)}")
    print(f"Total Articles with Climate Related News Found: {total_articles}")
    print("Articles Per Year:")
    for year, count in sorted(articles_per_year.items()):
        print(f"  {year}: {count} articles")

    with open("disaster_news.json", "w") as f:
        json.dump(all_results, f, indent=4)

    csv_file = "disaster_news.csv"
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.DictWriter(file, fieldnames=["Entity", "Type", "Title", "Source", "Trusted", "Date"])
        writer.writeheader()
        writer.writerows(csv_rows)

    logging.info(f"Scraping completed. Total entities analyzed: {len(all_results)}")
    logging.info(f"CSV file saved as {csv_file}")

if __name__ == "__main__":
    scrape_disaster_related_news()