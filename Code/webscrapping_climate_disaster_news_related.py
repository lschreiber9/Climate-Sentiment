# -*- coding: utf-8 -*-
"""Webscrapping Climate Disaster news related.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11ueDflTSSEa_0gZro1G-55VAN_mhTa-0

Test 15/12
"""

pip install geopandas matplotlib
!pip install scrapy
!pip install basemap basemap-data-hires

"""##Final Code##

**Intermediary Steps: Scrapping an Actual Website**
"""

from bs4 import BeautifulSoupIntermediary Steps: Scrapping an Actual Website


[ ]

import requests
import re
from collections import defaultdict

# Define disaster keywords and states
disaster_keywords = [
    "extreme heat", "heatwave", "heat dome", "urban heat island",
    "extreme cold", "polar vortex", "frost", "cold snap", "blizzard",
    "flood", "riverine flood", "urban flood", "coastal flood", "flash flood",
    "ice jam flood", "glacial lake outburst flood", "dam break flood", "mudslide",
    "wildfire", "forest fire", "bushfire", "peat fire", "urban wildfire", "wildfire smoke",
    "drought", "agricultural drought", "meteorological drought", "hydrological drought",
    "water scarcity", "desertification",
    "storm", "hurricane", "typhoon", "cyclone", "tornado", "hailstorm",
    "lightning storm", "derecho", "storm surge", "dust storm", "windstorm",
    "marine heatwave", "coral bleaching", "sea level rise", "coastal erosion",
    "permafrost thaw", "avalanche", "sea ice loss",
    "pest outbreaks", "disease outbreaks", "vector-borne diseases", "waterborne diseases",
    "fire-flood", "storm-fire", "drought-induced wildfire", "heatwave-power outage",
    "climate migration", "food insecurity", "economic losses"
]

states = [
    "Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado", "Connecticut", "Delaware", "Florida", "Georgia",
    "Hawaii", "Idaho", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", "Maine", "Maryland",
    "Massachusetts", "Michigan", "Minnesota", "Mississippi", "Missouri", "Montana", "Nebraska", "Nevada",
    "New Hampshire", "New Jersey", "New Mexico", "New York", "North Carolina", "North Dakota", "Ohio",
    "Oklahoma", "Oregon", "Pennsylvania", "Rhode Island", "South Carolina", "South Dakota", "Tennessee",
    "Texas", "Utah", "Vermont", "Virginia", "Washington", "West Virginia", "Wisconsin", "Wyoming"
]

# Target URL
url = "https://www.weather.gov/news/"

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

def get_relevant_headlines_by_state(debug=False):
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 403:
            print("Access denied: The server returned a 403 Forbidden error.")
            return defaultdict(list)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        titles = soup.find_all('a')  # Searching all <a> tags for possible headlines

        if not titles:
            print("No headlines found. Check the webpage structure.")
            return defaultdict(list)

        keyword_pattern = re.compile(r'|'.join(disaster_keywords), re.IGNORECASE)
        state_pattern = re.compile(r'|'.join(states), re.IGNORECASE)
        year_pattern = re.compile(r"\b(2020|2021|2022|2023)\b")  # Match years between 2020-2023

        classified_headlines = defaultdict(list)  # Dictionary to classify headlines by state
        for title in titles:
            headline = title.get_text(strip=True)
            if year_pattern.search(headline) and keyword_pattern.search(headline):
                matched_states = state_pattern.findall(headline)
                if matched_states:
                    for state in matched_states:
                        classified_headlines[state].append(headline)
                        if debug:
                            print(f"Matched headline: {headline} (State: {state})")

        return classified_headlines

    except requests.exceptions.HTTPError as http_err:
        print(f"HTTP error occurred: {http_err}")
    except Exception as err:
        print(f"An error occurred: {err}")

    return defaultdict(list)

# Get all relevant headlines classified by state
classified_headlines = get_relevant_headlines_by_state(debug=False)

if classified_headlines:
    print("\nRelevant Headlines (2020-2023) by State:")
    for state, headlines in classified_headlines.items():
        print(f"\n{state}:")
        for i, headline in enumerate(headlines, start=1):
            print(f"  {i}. {headline}")
else:
    print("No relevant headlines found between 2020 and 2023.")

"""**Final scrapper incorporating scrapping Google news + Real Website**"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import json
import csv
import time

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Disaster keywords with advanced pattern matching
disaster_keywords = [
    "extreme heat", "heatwave", "heat dome", "urban heat island",
    "extreme cold", "polar vortex", "frost", "cold snap", "blizzard",
    "flood", "riverine flood", "urban flood", "coastal flood", "flash flood",
    "ice jam flood", "glacial lake outburst flood", "dam break flood", "mudslide",
    "wildfire", "forest fire", "bushfire", "peat fire", "urban wildfire", "wildfire smoke",
    "drought", "agricultural drought", "meteorological drought", "hydrological drought",
    "water scarcity", "desertification",
    "storm", "hurricane", "typhoon", "cyclone", "tornado", "hailstorm",
    "lightning storm", "derecho", "storm surge", "dust storm", "windstorm",
    "marine heatwave", "coral bleaching", "sea level rise", "coastal erosion",
    "permafrost thaw", "avalanche", "sea ice loss",
    "pest outbreaks", "disease outbreaks", "vector-borne diseases", "waterborne diseases",
    "fire-flood", "storm-fire", "drought-induced wildfire", "heatwave-power outage",
    "climate migration", "food insecurity", "economic losses"
]
disaster_pattern = re.compile("|".join(disaster_keywords), re.IGNORECASE)

# State names
states = [
    "Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado", "Connecticut", "Delaware", "Florida", "Georgia",
    "Hawaii", "Idaho", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", "Maine", "Maryland",
    "Massachusetts", "Michigan", "Minnesota", "Mississippi", "Missouri", "Montana", "Nebraska", "Nevada",
    "New Hampshire", "New Jersey", "New Mexico", "New York", "North Carolina", "North Dakota", "Ohio",
    "Oklahoma", "Oregon", "Pennsylvania", "Rhode Island", "South Carolina", "South Dakota", "Tennessee",
    "Texas", "Utah", "Vermont", "Virginia", "Washington", "West Virginia", "Wisconsin", "Wyoming"
]

# Common company suffixes for cleaning
common_suffixes = [
    " Inc", " Corp", " plc", " Co", " Ltd", " LLC", " Group", " Holdings", " Intl",
    " Incorporated", " Company", " Limited", " Enterprises", " Technologies", " Solutions"
]

# Full list of companies and cleaned versions
companies = {
    company: re.sub(f"({'|'.join(common_suffixes)})[.,]?$", "", company, flags=re.IGNORECASE).strip()
    for company in [
        "GE Aerospace", "Caterpillar Inc", "RTX Corporation", "Uber Technologies Inc.",
        "Union Pacific Corp", "Eaton Corp plc", "Honeywell Intl Inc", "Lockheed Martin",
        "Automatic Data Processing", "Deere & Co", "United Parcel Service Inc B", "Boeing Co",
        "Trane Technologies plc", "GE Vernova Inc", "Parker-Hannifin Corp", "General Dynamics",
        "TransDigm Group", "Waste Management Inc", "Northrop Grumman Corp", "Cintas Corp",
        "Illinois Tool Works Inc", "3M Co", "CSX Corporation", "Emerson Electric Co",
        "Carrier Global Corp.", "FedEx Corp", "Norfolk Southern Corp", "PACCAR Inc",
        "United Rentals Inc", "Johnson Controls International plc", "W.W. Grainger Inc",
        "L3Harris Technologies Inc", "Quanta Services Inc", "Copart Inc", "Cummins Inc",
        "Paychex Inc", "Fastenal Co", "Howmet Aerospace Inc.", "Republic Services Inc",
        "Otis Worldwide Corp", "Ingersoll Rand Inc.", "AMETEK Inc", "Verisk Analytics Inc",
        "Old Dominion Freight Line Inc", "Delta Air Lines", "Equifax Inc", "Wabtec",
        "Axon Enterprise Inc", "Xylem Inc", "Rockwell Automation Inc", "Fortive Corp",
        "Veralto Corp", "Dover Corp", "Broadridge Financial Solutions Inc.",
        "United Airlines Holding, Inc", "Hubbell Inc", "Leidos Holdings Inc", "Builders FirstSource",
        "Masco Corp", "Southwest Airlines Co", "Jacobs Solutions Inc.", "Snap On Inc",
        "Expeditors Intl of WA Inc", "Pentair PLC", "Stanley Black & Decker", "Textron Inc",
        "IDEX Corp", "J.B. Hunt Transport Services", "Nordson Corp", "Rollins Inc",
        "CH Robinson Worldwide Inc", "Allegion plc", "Dayforce, Inc.", "Huntington Ingalls Industries Inc.",
        "Generac Holdings Inc", "A.O. Smith Corp", "Paycom Software Inc", "Amentum Holdings Inc."
    ]
}

def scrape_google_news(query):
    logging.info(f"Scraping Google News for query: {query}")
    results = []
    rss_url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"

    try:
        response = requests.get(rss_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "xml")

        items = soup.find_all("item")
        seen_titles = set()

        for item in items:
            title = item.title.text.strip()
            description = item.description.text.strip() if item.description else "No description available"
            source = item.source.text.strip() if item.source else "Unknown"
            pub_date = item.pubDate.text if item.pubDate else None

            # Remove duplicates
            if title.lower() in seen_titles:
                continue
            seen_titles.add(title.lower())

            # Check publication date (2020–2023)
            if pub_date:
                pub_date_obj = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %Z")
                if not (datetime(2020, 1, 1) <= pub_date_obj <= datetime(2023, 12, 31)):
                    continue
            else:
                continue

            # Check if disaster keywords are in title or description
            if not (disaster_pattern.search(title) or disaster_pattern.search(description)):
                continue

            # Add filtered results
            results.append({
                "title": title,
                "description": description,
                "source": source,
                "is_trusted": source.lower() in [
                    "bbc.com", "the washington post", "the guardian", "reuters",
                    "the new york times", "bloomberg", "the associated press", "cnn",
                    "al jazeera", "npr", "financial times", "the economist",
                    "nature.com", "national geographic", "scientific american",
                    "insideclimate news", "carbon brief", "climate central",
                    "south china morning post", "the straits times"
                ],
                "date": pub_date_obj.strftime("%Y-%m-%d")
            })

    except Exception as e:
        logging.error(f"Error scraping Google News for query '{query}': {e}")

    return results

def scrape_disaster_related_news():
    all_results = {}

    with ThreadPoolExecutor(max_workers=10) as executor:
        state_futures = {}
        company_futures = {}

        # Company queries
        for company, cleaned_name in companies.items():
            query = f"{cleaned_name} climate disaster OR {cleaned_name} extreme weather OR {cleaned_name} climate impact"
            company_futures[executor.submit(scrape_google_news, query)] = company

        # State-level queries with delay for Florida and New York
        for state in states:
            query = f"{state} climate disaster OR {state} extreme weather OR {state} climate impact"

            if state in ["Florida", "New York"]:
                logging.info(f"Adding delay before scraping for {state}")
                time.sleep(5)  # Wait for 5 seconds

            state_futures[executor.submit(scrape_google_news, query)] = state

        for future in as_completed(state_futures):
            results = future.result()
            key = state_futures[future]

            if results:
                if key not in all_results:
                    all_results[key] = []
                all_results[key].extend(results)

        for future in as_completed(company_futures):
            results = future.result()
            key = company_futures[future]

            if results:
                if key not in all_results:
                    all_results[key] = []
                all_results[key].extend(results)

    # Identify entities with no climate news
    no_news_states = [state for state in states if state not in all_results]
    no_news_companies = [company for company in companies if company not in all_results]

    # Log entities with no news
    logging.info(f"States with no climate news: {', '.join(no_news_states)}")
    logging.info(f"Companies with no climate news: {', '.join(no_news_companies)}")

    # Print missing states with climate-related news
    print(f"\nMissing states with climate related news = {', '.join(no_news_states)}")

    # Print results to console
    for key, results in all_results.items():
        print(f"\n{'Company' if key in companies else 'State'}: {key}")
        for result in results:
            trusted_status = "Trusted Source" if result["is_trusted"] else "Untrusted Source"
            print(f"  - {result['title']} - {result['source']} ({trusted_status}, Published: {result['date']})")

    # Summary statistics
    company_count = sum(1 for key in all_results if key in companies)
    state_count = sum(1 for key in all_results if key in states)
    total_articles = sum(len(results) for results in all_results.values())

    articles_per_year = {}
    csv_rows = []
    for key, results in all_results.items():
        for result in results:
            year = result['date'][:4]
            articles_per_year[year] = articles_per_year.get(year, 0) + 1

            csv_rows.append({
                "Entity": key,
                "Type": "Company" if key in companies else "State",
                "Title": result['title'],
                "Source": result['source'],
                "Trusted": "Yes" if result['is_trusted'] else "No",
                "Date": result['date']
            })

    print("\nSummary Statistics:")
    print(f"Total Companies with Climate Related News: {company_count} out of {len(companies)}")
    print(f"Total States with Climate Related News: {state_count} out of {len(states)}")
    print(f"Total Articles with Climate Related News Found: {total_articles}")
    print("Articles Per Year:")
    for year, count in sorted(articles_per_year.items()):
        print(f"  {year}: {count} articles")

    # Save results to JSON for further analysis
    with open("disaster_news.json", "w") as f:
        json.dump(all_results, f, indent=4)

    # Save results to CSV for QGIS
    csv_file = "disaster_news.csv"
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.DictWriter(file, fieldnames=["Entity", "Type", "Title", "Source", "Trusted", "Date"])
        writer.writeheader()
        writer.writerows(csv_rows)

    logging.info(f"Scraping completed. Total entities analyzed: {len(all_results)}")
    logging.info(f"CSV file saved as {csv_file}")

if __name__ == "__main__":
    scrape_disaster_related_news()

"""##DRAFT##

Plotting on a map
"""

import geopandas as gpd
import matplotlib.pyplot as plt
from collections import defaultdict
import csv
import os

# Function to plot articles on a map
def plot_articles_on_map():
    # Load the CSV file created by scrape_disaster_related_news()
    csv_file = "disaster_news_combined.csv"
    state_articles = defaultdict(int)

    # Check if CSV file exists
    if not os.path.exists(csv_file):
        print(f"CSV file '{csv_file}' not found. Ensure scrape_disaster_related_news() was executed.")
        return

    try:
        with open(csv_file, mode="r", encoding="utf-8") as file:
            reader = csv.DictReader(file)
            for row in reader:
                if row["Type"] == "State" and row["Entity"] in states:
                    state_articles[row["Entity"]] += 1
    except Exception as e:
        print(f"Error reading CSV file: {e}")
        return

    # Debug: Print collected article counts
    print("State article counts:")
    print(dict(state_articles))

    # Load US states shapefile (Natural Earth via Geopandas)
    try:
        us_states = gpd.read_file(gpd.datasets.get_path("naturalearth_lowres"))
        # Filter for the United States
        us_states = us_states[us_states["iso_a2"] == "US"]
    except Exception as e:
        print(f"Error loading US states shapefile: {e}")
        return

    # Map article counts to GeoDataFrame
    us_states["article_count"] = us_states["name"].map(state_articles).fillna(0)

    # Debug: Print GeoDataFrame with article counts
    print("GeoDataFrame with article counts:")
    print(us_states[["name", "article_count"]])

    # Plot the map
    try:
        fig, ax = plt.subplots(1, 1, figsize=(15, 10))
        us_states.boundary.plot(ax=ax, linewidth=1, color="black")
        us_states.plot(
            column="article_count",
            ax=ax,
            legend=True,
            legend_kwds={"label": "Number of Climate News Articles", "orientation": "vertical"},
            cmap="Blues",
            edgecolor="black"
        )

        # Add title
        plt.title("Climate-Related News Articles (2020-2023) by State", fontsize=16)
        plt.axis("off")
        plt.show()
    except Exception as e:
        print(f"Error while plotting the map: {e}")

# Call the function to plot the map
plot_articles_on_map()

"""###Final Model Scrapping
  **Summary of Code Implementation**:

  - **Scraping Data:**
    - Implemented the `scrape_google_news` function to fetch articles from Google News RSS feeds using disaster-related keywords.
    - Filtered articles based on keywords, publication dates (2020–2023), and source trustworthiness.

  - **Entity Classification:**
    - Queried news articles for both companies and U.S. states separately.
    - Identified and categorized results as either "Company" or "State."

  - **Result Organization:**
    - Consolidated results into a structured dictionary (`all_results`) with entities as keys and articles as values.

  - **Output Statistics:**
    - Counted the number of companies and states with climate-related news.
    - Computed the total number of articles found across all entities.
    - Analyzed and summarized articles by year.

  - **Data Export:**
    - Saved results to a JSON file (`disaster_news.json`) for further analysis.
    - Exported results to a CSV file (`disaster_news.csv`) formatted for use with tools like QGIS, including fields for entity name, type, title, source, trust status, and publication date.

  **Summary Statistics:**

  - **Total Companies with Climate Related News:** 38 out of 78
  - **Total States with Climate Related News:** 46 out of 50
  - **Total Articles with Climate Related News Found:** 425

  **Articles Per Year:**
  - 2020: 15 articles
  - 2021: 86 articles
  - 2022: 88 articles
  - 2023: 236 articles
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import json
import csv

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Disaster keywords with advanced pattern matching
disaster_keywords = [
    "extreme heat", "heatwave", "heat dome", "urban heat island",
    "extreme cold", "polar vortex", "frost", "cold snap", "blizzard",
    "flood", "riverine flood", "urban flood", "coastal flood", "flash flood",
    "ice jam flood", "glacial lake outburst flood", "dam break flood", "mudslide",
    "wildfire", "forest fire", "bushfire", "peat fire", "urban wildfire", "wildfire smoke",
    "drought", "agricultural drought", "meteorological drought", "hydrological drought",
    "water scarcity", "desertification",
    "storm", "hurricane", "typhoon", "cyclone", "tornado", "hailstorm",
    "lightning storm", "derecho", "storm surge", "dust storm", "windstorm",
    "marine heatwave", "coral bleaching", "sea level rise", "coastal erosion",
    "permafrost thaw", "avalanche", "sea ice loss",
    "pest outbreaks", "disease outbreaks", "vector-borne diseases", "waterborne diseases",
    "fire-flood", "storm-fire", "drought-induced wildfire", "heatwave-power outage",
    "climate migration", "food insecurity", "economic losses"
]
disaster_pattern = re.compile("|".join(disaster_keywords), re.IGNORECASE)

# State names
states = [
    "Alabama", "Alaska", "Arizona", "Arkansas", "California", "Colorado", "Connecticut", "Delaware", "Florida", "Georgia",
    "Hawaii", "Idaho", "Illinois", "Indiana", "Iowa", "Kansas", "Kentucky", "Louisiana", "Maine", "Maryland",
    "Massachusetts", "Michigan", "Minnesota", "Mississippi", "Missouri", "Montana", "Nebraska", "Nevada",
    "New Hampshire", "New Jersey", "New Mexico", "New York", "North Carolina", "North Dakota", "Ohio",
    "Oklahoma", "Oregon", "Pennsylvania", "Rhode Island", "South Carolina", "South Dakota", "Tennessee",
    "Texas", "Utah", "Vermont", "Virginia", "Washington", "West Virginia", "Wisconsin", "Wyoming"
]

# Common company suffixes for cleaning
common_suffixes = [
    " Inc", " Corp", " plc", " Co", " Ltd", " LLC", " Group", " Holdings", " Intl",
    " Incorporated", " Company", " Limited", " Enterprises", " Technologies", " Solutions"
]

# Full list of companies and cleaned versions
companies = {
    company: re.sub(f"({'|'.join(common_suffixes)})[.,]?$", "", company, flags=re.IGNORECASE).strip()
    for company in [
        "GE Aerospace", "Caterpillar Inc", "RTX Corporation", "Uber Technologies Inc.",
        "Union Pacific Corp", "Eaton Corp plc", "Honeywell Intl Inc", "Lockheed Martin",
        "Automatic Data Processing", "Deere & Co", "United Parcel Service Inc B", "Boeing Co",
        "Trane Technologies plc", "GE Vernova Inc", "Parker-Hannifin Corp", "General Dynamics",
        "TransDigm Group", "Waste Management Inc", "Northrop Grumman Corp", "Cintas Corp",
        "Illinois Tool Works Inc", "3M Co", "CSX Corporation", "Emerson Electric Co",
        "Carrier Global Corp.", "FedEx Corp", "Norfolk Southern Corp", "PACCAR Inc",
        "United Rentals Inc", "Johnson Controls International plc", "W.W. Grainger Inc",
        "L3Harris Technologies Inc", "Quanta Services Inc", "Copart Inc", "Cummins Inc",
        "Paychex Inc", "Fastenal Co", "Howmet Aerospace Inc.", "Republic Services Inc",
        "Otis Worldwide Corp", "Ingersoll Rand Inc.", "AMETEK Inc", "Verisk Analytics Inc",
        "Old Dominion Freight Line Inc", "Delta Air Lines", "Equifax Inc", "Wabtec",
        "Axon Enterprise Inc", "Xylem Inc", "Rockwell Automation Inc", "Fortive Corp",
        "Veralto Corp", "Dover Corp", "Broadridge Financial Solutions Inc.",
        "United Airlines Holding, Inc", "Hubbell Inc", "Leidos Holdings Inc", "Builders FirstSource",
        "Masco Corp", "Southwest Airlines Co", "Jacobs Solutions Inc.", "Snap On Inc",
        "Expeditors Intl of WA Inc", "Pentair PLC", "Stanley Black & Decker", "Textron Inc",
        "IDEX Corp", "J.B. Hunt Transport Services", "Nordson Corp", "Rollins Inc",
        "CH Robinson Worldwide Inc", "Allegion plc", "Dayforce, Inc.", "Huntington Ingalls Industries Inc.",
        "Generac Holdings Inc", "A.O. Smith Corp", "Paycom Software Inc", "Amentum Holdings Inc."
    ]
}

def scrape_google_news(query):
    logging.info(f"Scraping Google News for query: {query}")
    results = []
    rss_url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"

    try:
        response = requests.get(rss_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "xml")

        items = soup.find_all("item")
        seen_titles = set()

        for item in items:
            title = item.title.text.strip()
            description = item.description.text.strip() if item.description else "No description available"
            source = item.source.text.strip() if item.source else "Unknown"
            pub_date = item.pubDate.text if item.pubDate else None

            # Remove duplicates
            if title.lower() in seen_titles:
                continue
            seen_titles.add(title.lower())

            # Check publication date (2020–2023)
            if pub_date:
                pub_date_obj = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %Z")
                if not (datetime(2020, 1, 1) <= pub_date_obj <= datetime(2023, 12, 31)):
                    continue
            else:
                continue

            # Check if disaster keywords are in title or description
            if not (disaster_pattern.search(title) or disaster_pattern.search(description)):
                continue

            # Add filtered results
            results.append({
                "title": title,
                "description": description,
                "source": source,
                "is_trusted": source.lower() in [
                    "bbc.com", "the washington post", "the guardian", "reuters",
                    "the new york times", "bloomberg", "the associated press", "cnn",
                    "al jazeera", "npr", "financial times", "the economist",
                    "nature.com", "national geographic", "scientific american",
                    "insideclimate news", "carbon brief", "climate central",
                    "south china morning post", "the straits times"
                ],
                "date": pub_date_obj.strftime("%Y-%m-%d")
            })

    except Exception as e:
        logging.error(f"Error scraping Google News for query '{query}': {e}")

    return results

def scrape_disaster_related_news():
    all_results = {}

    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {}

        # Company queries
        for company, cleaned_name in companies.items():
            query = f"{cleaned_name} climate disaster OR {cleaned_name} extreme weather OR {cleaned_name} climate impact"
            futures[executor.submit(scrape_google_news, query)] = company

        # State-level queries
        for state in states:
            query = f"{state} climate disaster OR {state} extreme weather OR {state} climate impact"
            futures[executor.submit(scrape_google_news, query)] = state

        for future in as_completed(futures):
            results = future.result()
            key = futures[future]

            if results:
                if key not in all_results:
                    all_results[key] = []
                all_results[key].extend(results)

    # Print results to console
    for key, results in all_results.items():
        print(f"\n{'Company' if key in companies else 'State'}: {key}")
        for result in results:
            trusted_status = "Trusted Source" if result["is_trusted"] else "Untrusted Source"
            print(f"  - {result['title']} - {result['source']} ({trusted_status}, Published: {result['date']})")

    # Summary statistics
    company_count = sum(1 for key in all_results if key in companies)
    state_count = sum(1 for key in all_results if key in states)
    total_articles = sum(len(results) for results in all_results.values())

    articles_per_year = {}
    csv_rows = []
    for key, results in all_results.items():
        for result in results:
            year = result['date'][:4]
            articles_per_year[year] = articles_per_year.get(year, 0) + 1

            csv_rows.append({
                "Entity": key,
                "Type": "Company" if key in companies else "State",
                "Title": result['title'],
                "Source": result['source'],
                "Trusted": "Yes" if result['is_trusted'] else "No",
                "Date": result['date']
            })

    print("\nSummary Statistics:")
    print(f"Total Companies with Climate Related News: {company_count} out of {len(companies)}")
    print(f"Total States with Climate Related News: {state_count} out of {len(states)}")
    print(f"Total Articles with Climate Related News Found: {total_articles}")
    print("Articles Per Year:")
    for year, count in sorted(articles_per_year.items()):
        print(f"  {year}: {count} articles")

    # Save results to JSON for further analysis
    with open("disaster_news.json", "w") as f:
        json.dump(all_results, f, indent=4)

    # Save results to CSV for QGIS
    csv_file = "disaster_news.csv"
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.DictWriter(file, fieldnames=["Entity", "Type", "Title", "Source", "Trusted", "Date"])
        writer.writeheader()
        writer.writerows(csv_rows)

    logging.info(f"Scraping completed. Total entities analyzed: {len(all_results)}")
    logging.info(f"CSV file saved as {csv_file}")

if __name__ == "__main__":
    scrape_disaster_related_news()

"""### Old Models

**First Scrapping Model**

*   Conclusion:
    * Too much noise
    * Not taking into account the company name
    * Didn't include the date
    * But lot of news

*   Summary Statistics:
  * Total companies analyzed: 78
  * Companies with disaster-related news: 76
  * Companies without disaster-related news: 2
"""

import requests
from bs4 import BeautifulSoup

def scrape_news_headlines():
    companies = [
        "GE Aerospace", "Caterpillar Inc", "RTX Corporation", "Uber Technologies Inc.", "Union Pacific Corp",
        "Eaton Corp plc", "Honeywell Intl Inc", "Lockheed Martin", "Automatic Data Processing", "Deere & Co",
        "United Parcel Service Inc B", "Boeing Co", "Trane Technologies plc", "GE Vernova Inc", "Parker-Hannifin Corp",
        "General Dynamics", "TransDigm Group", "Waste Management Inc", "Northrop Grumman Corp", "Cintas Corp",
        "Illinois Tool Works Inc", "3M Co", "CSX Corporation", "Emerson Electric Co", "Carrier Global Corp.",
        "FedEx Corp", "Norfolk Southern Corp", "PACCAR Inc", "United Rentals Inc", "Johnson Controls International plc",
        "W.W. Grainger Inc", "L3Harris Technologies Inc", "Quanta Services Inc", "Copart Inc", "Cummins Inc",
        "Paychex Inc", "Fastenal Co", "Howmet Aerospace Inc.", "Republic Services Inc", "Otis Worldwide Corp",
        "Ingersoll Rand Inc.", "AMETEK Inc", "Verisk Analytics Inc", "Old Dominion Freight Line Inc", "Delta Air Lines",
        "Equifax Inc", "Wabtec", "Axon Enterprise Inc", "Xylem Inc", "Rockwell Automation Inc", "Fortive Corp",
        "Veralto Corp", "Dover Corp", "Broadridge Financial Solutions Inc.", "United Airlines Holding, Inc",
        "Hubbell Inc", "Leidos Holdings Inc", "Builders FirstSource", "Masco Corp", "Southwest Airlines Co",
        "Jacobs Solutions Inc.", "Snap On Inc", "Expeditors Intl of WA Inc", "Pentair PLC", "Stanley Black & Decker",
        "Textron Inc", "IDEX Corp", "J.B. Hunt Transport Services", "Nordson Corp", "Rollins Inc",
        "CH Robinson Worldwide Inc", "Allegion plc", "Dayforce, Inc.", "Huntington Ingalls Industries Inc.",
        "Generac Holdings Inc", "A.O. Smith Corp", "Paycom Software Inc", "Amentum Holdings Inc."
    ]

    results = []
    companies_with_news = 0
    companies_without_news = 0

    for company in companies:
        try:
            print(f"Scraping news for: {company}")
            query = f"{company} climate disaster"
            rss_url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"

            response = requests.get(rss_url)
            response.raise_for_status()  # Ensure the request was successful
            soup = BeautifulSoup(response.content, "xml")

            # Extract news headlines
            items = soup.find_all("item")
            headlines = [item.title.text for item in items]

            if headlines:
                companies_with_news += 1
            else:
                companies_without_news += 1

            results.append({
                "company": company,
                "headlines": headlines
            })

        except Exception as e:
            print(f"Error scraping news for {company}: {e}")
            companies_without_news += 1

    # Output results
    print("\nDetailed Results:")
    for result in results:
        print(f"Company: {result['company']}")
        if result["headlines"]:
            for headline in result["headlines"]:
                print(f"  - {headline}")
        else:
            print("  - No news found")

    # Summary statistics
    total_companies = len(companies)
    print("\nSummary:")
    print(f"Total companies analyzed: {total_companies}")
    print(f"Companies with disaster-related news: {companies_with_news}")
    print(f"Companies without disaster-related news: {companies_without_news}")

if __name__ == "__main__":
    scrape_news_headlines()

"""**Second Model**
* Key caracteristics:
    * Problem with the company form name
    * noise reduced


*   Summary Statistics:
  * Total companies analyzed: 78
  * Companies with disaster-related news: 22
  * Companies without disaster-related news: 56




"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime

def scrape_disaster_related_news():
    companies = [
        "GE Aerospace", "Caterpillar Inc", "RTX Corporation", "Uber Technologies Inc.",
        "Union Pacific Corp", "Eaton Corp plc", "Honeywell Intl Inc", "Lockheed Martin",
        "Automatic Data Processing", "Deere & Co", "United Parcel Service Inc B", "Boeing Co",
        "Trane Technologies plc", "GE Vernova Inc", "Parker-Hannifin Corp", "General Dynamics",
        "TransDigm Group", "Waste Management Inc", "Northrop Grumman Corp", "Cintas Corp",
        "Illinois Tool Works Inc", "3M Co", "CSX Corporation", "Emerson Electric Co",
        "Carrier Global Corp.", "FedEx Corp", "Norfolk Southern Corp", "PACCAR Inc",
        "United Rentals Inc", "Johnson Controls International plc", "W.W. Grainger Inc",
        "L3Harris Technologies Inc", "Quanta Services Inc", "Copart Inc", "Cummins Inc",
        "Paychex Inc", "Fastenal Co", "Howmet Aerospace Inc.", "Republic Services Inc",
        "Otis Worldwide Corp", "Ingersoll Rand Inc.", "AMETEK Inc", "Verisk Analytics Inc",
        "Old Dominion Freight Line Inc", "Delta Air Lines", "Equifax Inc", "Wabtec",
        "Axon Enterprise Inc", "Xylem Inc", "Rockwell Automation Inc", "Fortive Corp",
        "Veralto Corp", "Dover Corp", "Broadridge Financial Solutions Inc.",
        "United Airlines Holding, Inc", "Hubbell Inc", "Leidos Holdings Inc", "Builders FirstSource",
        "Masco Corp", "Southwest Airlines Co", "Jacobs Solutions Inc.", "Snap On Inc",
        "Expeditors Intl of WA Inc", "Pentair PLC", "Stanley Black & Decker", "Textron Inc",
        "IDEX Corp", "J.B. Hunt Transport Services", "Nordson Corp", "Rollins Inc",
        "CH Robinson Worldwide Inc", "Allegion plc", "Dayforce, Inc.", "Huntington Ingalls Industries Inc.",
        "Generac Holdings Inc", "A.O. Smith Corp", "Paycom Software Inc", "Amentum Holdings Inc."
    ]

    disaster_keywords = [
        "extreme heat", "heatwave", "heat dome", "urban heat island", "extreme cold",
        "polar vortex", "frost", "cold snap", "blizzard", "flood", "riverine flood",
        "urban flood", "coastal flood", "flash flood", "ice jam flood",
        "glacial lake outburst flood", "dam break flood", "mudslide", "wildfire",
        "forest fire", "bushfire", "peat fire", "urban wildfire", "wildfire smoke",
        "drought", "agricultural drought", "meteorological drought", "hydrological drought",
        "water scarcity", "desertification", "storm", "hurricane", "typhoon",
        "cyclone", "tornado", "hailstorm", "lightning storm", "derecho", "storm surge",
        "dust storm", "windstorm", "marine heatwave", "coral bleaching", "sea level rise",
        "coastal erosion", "permafrost thaw", "avalanche", "sea ice loss", "pest outbreaks",
        "disease outbreaks", "vector-borne diseases", "waterborne diseases", "fire-flood",
        "storm-fire", "drought-induced wildfire", "heatwave-power outage", "climate migration",
        "food insecurity", "economic losses"
    ]

    results = []
    companies_with_news = 0

    for company in companies:
        try:
            print(f"Scraping news for: {company}")
            query = f"{company} climate disaster"
            rss_url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"

            response = requests.get(rss_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "xml")

            items = soup.find_all("item")
            filtered_headlines = []

            for item in items:
                title = item.title.text.lower()
                pub_date = item.pubDate.text if item.pubDate else None

                if pub_date:
                    pub_date_obj = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %Z")
                    if 2020 <= pub_date_obj.year <= 2023:
                        if any(keyword in title for keyword in disaster_keywords):
                            filtered_headlines.append({
                                "title": item.title.text,
                                "date": pub_date_obj.strftime("%Y-%m-%d")
                            })

            if filtered_headlines:
                results.append({
                    "company": company,
                    "headlines": filtered_headlines
                })
                companies_with_news += 1

        except Exception as e:
            print(f"Error scraping news for {company}: {e}")

    # Summary statistics
    total_companies = len(companies)
    companies_without_news = total_companies - companies_with_news

    print("\nSummary Statistics:")
    print(f"  * Total companies analyzed: {total_companies}")
    print(f"  * Companies with disaster-related news: {companies_with_news}")
    print(f"  * Companies without disaster-related news: {companies_without_news}")

    # Output results
    for result in results:
        print(f"\nCompany: {result['company']}")
        for headline in result["headlines"]:
            print(f"  - {headline['title']} (Published: {headline['date']})")

if __name__ == "__main__":
    scrape_disaster_related_news()

import requests
from bs4 import BeautifulSoup
from datetime import datetime

def scrape_disaster_related_news():
    companies = [
        "GE Aerospace", "Caterpillar Inc", "RTX Corporation", "Uber Technologies Inc.",
        "Union Pacific Corp", "Eaton Corp plc", "Honeywell Intl Inc", "Lockheed Martin",
        "Automatic Data Processing", "Deere & Co", "United Parcel Service Inc B", "Boeing Co",
        "Trane Technologies plc", "GE Vernova Inc", "Parker-Hannifin Corp", "General Dynamics",
        "TransDigm Group", "Waste Management Inc", "Northrop Grumman Corp", "Cintas Corp",
        "Illinois Tool Works Inc", "3M Co", "CSX Corporation", "Emerson Electric Co",
        "Carrier Global Corp.", "FedEx Corp", "Norfolk Southern Corp", "PACCAR Inc",
        "United Rentals Inc", "Johnson Controls International plc", "W.W. Grainger Inc",
        "L3Harris Technologies Inc", "Quanta Services Inc", "Copart Inc", "Cummins Inc",
        "Paychex Inc", "Fastenal Co", "Howmet Aerospace Inc.", "Republic Services Inc",
        "Otis Worldwide Corp", "Ingersoll Rand Inc.", "AMETEK Inc", "Verisk Analytics Inc",
        "Old Dominion Freight Line Inc", "Delta Air Lines", "Equifax Inc", "Wabtec",
        "Axon Enterprise Inc", "Xylem Inc", "Rockwell Automation Inc", "Fortive Corp",
        "Veralto Corp", "Dover Corp", "Broadridge Financial Solutions Inc.",
        "United Airlines Holding, Inc", "Hubbell Inc", "Leidos Holdings Inc", "Builders FirstSource",
        "Masco Corp", "Southwest Airlines Co", "Jacobs Solutions Inc.", "Snap On Inc",
        "Expeditors Intl of WA Inc", "Pentair PLC", "Stanley Black & Decker", "Textron Inc",
        "IDEX Corp", "J.B. Hunt Transport Services", "Nordson Corp", "Rollins Inc",
        "CH Robinson Worldwide Inc", "Allegion plc", "Dayforce, Inc.", "Huntington Ingalls Industries Inc.",
        "Generac Holdings Inc", "A.O. Smith Corp", "Paycom Software Inc", "Amentum Holdings Inc."
    ]

    disaster_keywords = [
    # Extreme temperatures
    "extreme heat",  # Heatwaves, heat domes, urban heat islands
    "heatwave",  # Prolonged periods of excessively hot weather
    "heat dome",  # High-pressure systems trapping heat
    "urban heat island",  # Cities experiencing elevated temperatures
    "extreme cold",  # Polar vortex, frost, cold snaps, blizzards
    "polar vortex",  # Large-scale cyclone of cold Arctic air
    "frost",  # Freezing temperatures damaging crops
    "cold snap",  # Sudden drop in temperature
    "blizzard",  # Severe snowstorms with high winds

    # Flooding
    "flood",  # General flooding events
    "riverine flood",  # Overflowing rivers
    "urban flood",  # Overwhelmed drainage in cities
    "coastal flood",  # Flooding from sea level rise or storm surges
    "flash flood",  # Sudden, short-duration flooding
    "ice jam flood",  # Caused by ice blockages in rivers
    "glacial lake outburst flood",  # GLOF from melting glaciers
    "dam break flood",  # Infrastructure failure-induced floods
    "mudslide",  # Rain-induced landslides

    # Wildfires
    "wildfire",  # Includes various types of uncontrolled fires
    "forest fire",  # Fires in forested areas
    "bushfire",  # Wildfires in bushlands
    "peat fire",  # Underground fires in peatlands
    "urban wildfire",  # Wildfires affecting urban areas
    "wildfire smoke",  # Health hazards from drifting smoke

    # Droughts and water scarcity
    "drought",  # General drought events
    "agricultural drought",  # Drought impacting crops and livestock
    "meteorological drought",  # Prolonged periods of low precipitation
    "hydrological drought",  # Dry rivers, lakes, and aquifers
    "water scarcity",  # Seasonal or chronic water shortages
    "desertification",  # Land degradation turning areas into desert

    # Storms
    "storm",  # General storm events
    "hurricane",  # Tropical cyclones in the Atlantic
    "typhoon",  # Tropical cyclones in the Pacific
    "cyclone",  # General term for tropical storms
    "tornado",  # High-intensity wind systems
    "hailstorm",  # Severe storms producing hailstones
    "lightning storm",  # Severe electrical storms
    "derecho",  # Long-lived, fast-moving windstorms
    "storm surge",  # Coastal flooding caused by storms
    "dust storm",  # Wind-driven particulate events
    "windstorm",  # High winds without precipitation

    # Ocean-related impacts
    "marine heatwave",  # Sudden extreme sea temperature increases
    "coral bleaching",  # Ecosystem collapse due to warming seas
    "sea level rise",  # Gradual increase in sea levels
    "coastal erosion",  # Loss of coastline due to rising seas

    # Cryosphere impacts
    "permafrost thaw",  # Methane release and infrastructure collapse
    "avalanche",  # Snow or ice slides
    "sea ice loss",  # Arctic and Antarctic ecosystem impacts

    # Biological impacts
    "pest outbreaks",  # Insects thriving under warming conditions
    "disease outbreaks",  # Malaria, dengue, and waterborne diseases
    "vector-borne diseases",  # Spread by mosquitoes, ticks, etc.
    "waterborne diseases",  # Cholera, leptospirosis, and others

    # Compound and cascading disasters
    "fire-flood",  # Post-wildfire floods and landslides
    "storm-fire",  # Fires ignited by lightning storms
    "drought-induced wildfire",  # Vegetation dryness-related wildfires
    "heatwave-power outage",  # Cascading urban heat and power loss

    # Societal impacts
    "climate migration",  # Social disaster resulting from displacement
    "food insecurity",  # Crop failure due to drought or flooding
    "economic losses",  # Infrastructure damage from climate disasters
]
    results = []

    for company in companies:
        try:
            print(f"Scraping news for: {company}")
            query = f"{company} climate disaster"
            rss_url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"

            response = requests.get(rss_url)
            response.raise_for_status()  # Ensure the request was successful
            soup = BeautifulSoup(response.content, "xml")

            # Extract and filter news headlines
            items = soup.find_all("item")
            filtered_headlines = []
            for item in items:
                title = item.title.text.lower()
                pub_date = item.pubDate.text if item.pubDate else None

                # Parse the publication date
                if pub_date:
                    pub_date_obj = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %Z")
                    if 2020 <= pub_date_obj.year <= 2023:
                        # Check if headline contains disaster keywords
                        if any(keyword in title for keyword in disaster_keywords):
                            filtered_headlines.append({
                                "title": item.title.text,
                                "date": pub_date_obj.strftime("%Y-%m-%d")
                            })

            if filtered_headlines:
                results.append({
                    "company": company,
                    "headlines": filtered_headlines
                })

        except Exception as e:
            print(f"Error scraping news for {company}: {e}")

    # Output results
    for result in results:
        print(f"Company: {result['company']}")
        for headline in result["headlines"]:
            print(f"  - {headline['title']} (Published: {headline['date']})")

if __name__ == "__main__":
    scrape_disaster_related_news()

"""**Third model**

* Key characteristics:
  * disaster keywords extended
  * years taken into account

* Summary statistics:
  * Total companies analyzed: 78
  * Companies with disaster-related news: 27
  * Companies without disaster-related news: 51
  
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import json

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Disaster keywords with advanced pattern matching
disaster_keywords = [
    "extreme heat", "heatwave", "heat dome", "urban heat island",
    "extreme cold", "polar vortex", "frost", "cold snap", "blizzard",
    "flood", "riverine flood", "urban flood", "coastal flood", "flash flood",
    "ice jam flood", "glacial lake outburst flood", "dam break flood", "mudslide",
    "wildfire", "forest fire", "bushfire", "peat fire", "urban wildfire", "wildfire smoke",
    "drought", "agricultural drought", "meteorological drought", "hydrological drought",
    "water scarcity", "desertification",
    "storm", "hurricane", "typhoon", "cyclone", "tornado", "hailstorm",
    "lightning storm", "derecho", "storm surge", "dust storm", "windstorm",
    "marine heatwave", "coral bleaching", "sea level rise", "coastal erosion",
    "permafrost thaw", "avalanche", "sea ice loss",
    "pest outbreaks", "disease outbreaks", "vector-borne diseases", "waterborne diseases",
    "fire-flood", "storm-fire", "drought-induced wildfire", "heatwave-power outage",
    "climate migration", "food insecurity", "economic losses"
]
disaster_pattern = re.compile("|".join(disaster_keywords), re.IGNORECASE)

# Full list of companies and cleaned versions
companies = {
    "GE Aerospace", "Caterpillar Inc", "RTX Corporation", "Uber Technologies Inc.",
    "Union Pacific Corp", "Eaton Corp plc", "Honeywell Intl Inc", "Lockheed Martin",
    "Automatic Data Processing", "Deere & Co", "United Parcel Service Inc B", "Boeing Co",
    "Trane Technologies plc", "GE Vernova Inc", "Parker-Hannifin Corp", "General Dynamics",
    "TransDigm Group", "Waste Management Inc", "Northrop Grumman Corp", "Cintas Corp",
    "Illinois Tool Works Inc", "3M Co", "CSX Corporation", "Emerson Electric Co",
    "Carrier Global Corp.", "FedEx Corp", "Norfolk Southern Corp", "PACCAR Inc",
    "United Rentals Inc", "Johnson Controls International plc", "W.W. Grainger Inc",
    "L3Harris Technologies Inc", "Quanta Services Inc", "Copart Inc", "Cummins Inc",
    "Paychex Inc", "Fastenal Co", "Howmet Aerospace Inc.", "Republic Services Inc",
    "Otis Worldwide Corp", "Ingersoll Rand Inc.", "AMETEK Inc", "Verisk Analytics Inc",
    "Old Dominion Freight Line Inc", "Delta Air Lines", "Equifax Inc", "Wabtec",
    "Axon Enterprise Inc", "Xylem Inc", "Rockwell Automation Inc", "Fortive Corp",
    "Veralto Corp", "Dover Corp", "Broadridge Financial Solutions Inc.",
    "United Airlines Holding, Inc", "Hubbell Inc", "Leidos Holdings Inc", "Builders FirstSource",
    "Masco Corp", "Southwest Airlines Co", "Jacobs Solutions Inc.", "Snap On Inc",
    "Expeditors Intl of WA Inc", "Pentair PLC", "Stanley Black & Decker", "Textron Inc",
    "IDEX Corp", "J.B. Hunt Transport Services", "Nordson Corp", "Rollins Inc",
    "CH Robinson Worldwide Inc", "Allegion plc", "Dayforce, Inc.", "Huntington Ingalls Industries Inc.",
    "Generac Holdings Inc", "A.O. Smith Corp", "Paycom Software Inc", "Amentum Holdings Inc."
}

def scrape_company_news(company):
    logging.info(f"Scraping news for: {company}")
    results = []

    query = f"{company} climate disaster OR {company} extreme weather OR {company} climate impact"
    rss_url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"

    try:
        response = requests.get(rss_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "xml")

        items = soup.find_all("item")
        for item in items:
            title = item.title.text
            pub_date = item.pubDate.text if item.pubDate else None

            if pub_date:
                pub_date_obj = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %Z")
                if 2020 <= pub_date_obj.year <= 2023:
                    if disaster_pattern.search(title):
                        results.append({
                            "title": title,
                            "date": pub_date_obj.strftime("%Y-%m-%d")
                        })

    except Exception as e:
        logging.error(f"Error scraping news for {company}: {e}")

    return {"company": company, "headlines": results} if results else None

def scrape_disaster_related_news():
    all_results = []

    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(scrape_company_news, company): company for company in companies}

        for future in as_completed(futures):
            result = future.result()
            if result:
                all_results.append(result)

    total_companies = len(companies)
    companies_with_news = len(all_results)
    companies_without_news = total_companies - companies_with_news

    # Summary statistics
    print("\nSummary Statistics:")
    print(f"  * Total companies analyzed: {total_companies}")
    print(f"  * Companies with disaster-related news: {companies_with_news}")
    print(f"  * Companies without disaster-related news: {companies_without_news}")

    # Save results to JSON for further analysis
    with open("disaster_news.json", "w") as f:
        json.dump(all_results, f, indent=4)

    # Output results
    for result in all_results:
        print(f"\nCompany: {result['company']}")
        for headline in result["headlines"]:
            print(f"  - {headline['title']} (Published: {headline['date']})")

if __name__ == "__main__":
    scrape_disaster_related_news()

"""Fourth Model
*

Summary Statistics:
  * Total companies analyzed: 78
  * Companies with disaster-related news: 28
  * Companies without disaster-related news: 50
  
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import json

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Disaster keywords with advanced pattern matching
disaster_keywords = [
    "extreme heat", "heatwave", "heat dome", "urban heat island",
    "extreme cold", "polar vortex", "frost", "cold snap", "blizzard",
    "flood", "riverine flood", "urban flood", "coastal flood", "flash flood",
    "ice jam flood", "glacial lake outburst flood", "dam break flood", "mudslide",
    "wildfire", "forest fire", "bushfire", "peat fire", "urban wildfire", "wildfire smoke",
    "drought", "agricultural drought", "meteorological drought", "hydrological drought",
    "water scarcity", "desertification",
    "storm", "hurricane", "typhoon", "cyclone", "tornado", "hailstorm",
    "lightning storm", "derecho", "storm surge", "dust storm", "windstorm",
    "marine heatwave", "coral bleaching", "sea level rise", "coastal erosion",
    "permafrost thaw", "avalanche", "sea ice loss",
    "pest outbreaks", "disease outbreaks", "vector-borne diseases", "waterborne diseases",
    "fire-flood", "storm-fire", "drought-induced wildfire", "heatwave-power outage",
    "climate migration", "food insecurity", "economic losses"
]
disaster_pattern = re.compile("|".join(disaster_keywords), re.IGNORECASE)

# Full list of companies and cleaned versions
companies = {
    "GE Aerospace", "Caterpillar Inc", "RTX Corporation", "Uber Technologies Inc.",
    "Union Pacific Corp", "Eaton Corp plc", "Honeywell Intl Inc", "Lockheed Martin",
    "Automatic Data Processing", "Deere & Co", "United Parcel Service Inc B", "Boeing Co",
    "Trane Technologies plc", "GE Vernova Inc", "Parker-Hannifin Corp", "General Dynamics",
    "TransDigm Group", "Waste Management Inc", "Northrop Grumman Corp", "Cintas Corp",
    "Illinois Tool Works Inc", "3M Co", "CSX Corporation", "Emerson Electric Co",
    "Carrier Global Corp.", "FedEx Corp", "Norfolk Southern Corp", "PACCAR Inc",
    "United Rentals Inc", "Johnson Controls International plc", "W.W. Grainger Inc",
    "L3Harris Technologies Inc", "Quanta Services Inc", "Copart Inc", "Cummins Inc",
    "Paychex Inc", "Fastenal Co", "Howmet Aerospace Inc.", "Republic Services Inc",
    "Otis Worldwide Corp", "Ingersoll Rand Inc.", "AMETEK Inc", "Verisk Analytics Inc",
    "Old Dominion Freight Line Inc", "Delta Air Lines", "Equifax Inc", "Wabtec",
    "Axon Enterprise Inc", "Xylem Inc", "Rockwell Automation Inc", "Fortive Corp",
    "Veralto Corp", "Dover Corp", "Broadridge Financial Solutions Inc.",
    "United Airlines Holding, Inc", "Hubbell Inc", "Leidos Holdings Inc", "Builders FirstSource",
    "Masco Corp", "Southwest Airlines Co", "Jacobs Solutions Inc.", "Snap On Inc",
    "Expeditors Intl of WA Inc", "Pentair PLC", "Stanley Black & Decker", "Textron Inc",
    "IDEX Corp", "J.B. Hunt Transport Services", "Nordson Corp", "Rollins Inc",
    "CH Robinson Worldwide Inc", "Allegion plc", "Dayforce, Inc.", "Huntington Ingalls Industries Inc.",
    "Generac Holdings Inc", "A.O. Smith Corp", "Paycom Software Inc", "Amentum Holdings Inc."
}

def scrape_company_news(company):
    logging.info(f"Scraping news for: {company}")
    results = []

    query = f"{company} climate disaster OR {company} extreme weather OR {company} climate impact"
    rss_url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"

    try:
        response = requests.get(rss_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "xml")

        items = soup.find_all("item")
        for item in items:
            title = item.title.text
            pub_date = item.pubDate.text if item.pubDate else None

            if pub_date:
                pub_date_obj = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %Z")
                if 2020 <= pub_date_obj.year <= 2023:
                    if disaster_pattern.search(title):
                        results.append({
                            "title": title,
                            "date": pub_date_obj.strftime("%Y-%m-%d")
                        })

    except Exception as e:
        logging.error(f"Error scraping news for {company}: {e}")

    return {"company": company, "headlines": results} if results else None

def scrape_disaster_related_news():
    all_results = []

    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(scrape_company_news, company): company for company in companies}

        for future in as_completed(futures):
            result = future.result()
            if result:
                all_results.append(result)

    total_companies = len(companies)
    companies_with_news = len(all_results)
    companies_without_news = total_companies - companies_with_news

    # Summary statistics
    print("\nSummary Statistics:")
    print(f"  * Total companies analyzed: {total_companies}")
    print(f"  * Companies with disaster-related news: {companies_with_news}")
    print(f"  * Companies without disaster-related news: {companies_without_news}")

    # Save results to JSON for further analysis
    with open("disaster_news.json", "w") as f:
        json.dump(all_results, f, indent=4)

    # Output results
    for result in all_results:
        print(f"\nCompany: {result['company']}")
        for headline in result["headlines"]:
            print(f"  - {headline['title']} (Published: {headline['date']})")

if __name__ == "__main__":
    scrape_disaster_related_news()

"""**Fifth model**
* Key characteristics:
  * trusted sources
  * years
  
* Summary Statistics:
  * Total companies analyzed: 78
  * Companies with disaster-related news: 28
  * Companies without disaster-related news: 50
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import json

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Disaster keywords with advanced pattern matching
disaster_keywords = [
    "extreme heat", "heatwave", "heat dome", "urban heat island",
    "extreme cold", "polar vortex", "frost", "cold snap", "blizzard",
    "flood", "riverine flood", "urban flood", "coastal flood", "flash flood",
    "ice jam flood", "glacial lake outburst flood", "dam break flood", "mudslide",
    "wildfire", "forest fire", "bushfire", "peat fire", "urban wildfire", "wildfire smoke",
    "drought", "agricultural drought", "meteorological drought", "hydrological drought",
    "water scarcity", "desertification",
    "storm", "hurricane", "typhoon", "cyclone", "tornado", "hailstorm",
    "lightning storm", "derecho", "storm surge", "dust storm", "windstorm",
    "marine heatwave", "coral bleaching", "sea level rise", "coastal erosion",
    "permafrost thaw", "avalanche", "sea ice loss",
    "pest outbreaks", "disease outbreaks", "vector-borne diseases", "waterborne diseases",
    "fire-flood", "storm-fire", "drought-induced wildfire", "heatwave-power outage",
    "climate migration", "food insecurity", "economic losses"
]
disaster_pattern = re.compile("|".join(disaster_keywords), re.IGNORECASE)

# Full list of companies and cleaned versions
companies = {
    "GE Aerospace", "Caterpillar Inc", "RTX Corporation", "Uber Technologies Inc.",
    "Union Pacific Corp", "Eaton Corp plc", "Honeywell Intl Inc", "Lockheed Martin",
    "Automatic Data Processing", "Deere & Co", "United Parcel Service Inc B", "Boeing Co",
    "Trane Technologies plc", "GE Vernova Inc", "Parker-Hannifin Corp", "General Dynamics",
    "TransDigm Group", "Waste Management Inc", "Northrop Grumman Corp", "Cintas Corp",
    "Illinois Tool Works Inc", "3M Co", "CSX Corporation", "Emerson Electric Co",
    "Carrier Global Corp.", "FedEx Corp", "Norfolk Southern Corp", "PACCAR Inc",
    "United Rentals Inc", "Johnson Controls International plc", "W.W. Grainger Inc",
    "L3Harris Technologies Inc", "Quanta Services Inc", "Copart Inc", "Cummins Inc",
    "Paychex Inc", "Fastenal Co", "Howmet Aerospace Inc.", "Republic Services Inc",
    "Otis Worldwide Corp", "Ingersoll Rand Inc.", "AMETEK Inc", "Verisk Analytics Inc",
    "Old Dominion Freight Line Inc", "Delta Air Lines", "Equifax Inc", "Wabtec",
    "Axon Enterprise Inc", "Xylem Inc", "Rockwell Automation Inc", "Fortive Corp",
    "Veralto Corp", "Dover Corp", "Broadridge Financial Solutions Inc.",
    "United Airlines Holding, Inc", "Hubbell Inc", "Leidos Holdings Inc", "Builders FirstSource",
    "Masco Corp", "Southwest Airlines Co", "Jacobs Solutions Inc.", "Snap On Inc",
    "Expeditors Intl of WA Inc", "Pentair PLC", "Stanley Black & Decker", "Textron Inc",
    "IDEX Corp", "J.B. Hunt Transport Services", "Nordson Corp", "Rollins Inc",
    "CH Robinson Worldwide Inc", "Allegion plc", "Dayforce, Inc.", "Huntington Ingalls Industries Inc.",
    "Generac Holdings Inc", "A.O. Smith Corp", "Paycom Software Inc", "Amentum Holdings Inc."
}

def scrape_company_news(company):
    logging.info(f"Scraping news for: {company}")
    results = []

    query = f"{company} climate disaster OR {company} extreme weather OR {company} climate impact"
    rss_url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"

    try:
        response = requests.get(rss_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "xml")

        items = soup.find_all("item")
        for item in items:
            title = item.title.text
            pub_date = item.pubDate.text if item.pubDate else None

            if pub_date:
                pub_date_obj = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %Z")
                if 2020 <= pub_date_obj.year <= 2023:
                    if disaster_pattern.search(title):
                        results.append({
                            "title": title,
                            "date": pub_date_obj.strftime("%Y-%m-%d")
                        })

    except Exception as e:
        logging.error(f"Error scraping news for {company}: {e}")

    return {"company": company, "headlines": results} if results else None

def scrape_disaster_related_news():
    all_results = []

    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(scrape_company_news, company): company for company in companies}

        for future in as_completed(futures):
            result = future.result()
            if result:
                all_results.append(result)

    total_companies = len(companies)
    companies_with_news = len(all_results)
    companies_without_news = total_companies - companies_with_news

    # Summary statistics
    print("\nSummary Statistics:")
    print(f"  * Total companies analyzed: {total_companies}")
    print(f"  * Companies with disaster-related news: {companies_with_news}")
    print(f"  * Companies without disaster-related news: {companies_without_news}")

    # Save results to JSON for further analysis
    with open("disaster_news.json", "w") as f:
        json.dump(all_results, f, indent=4)

    # Output results
    for result in all_results:
        print(f"\nCompany: {result['company']}")
        for headline in result["headlines"]:
            print(f"  - {headline['title']} (Published: {headline['date']})")

if __name__ == "__main__":
    scrape_disaster_related_news()

"""**Final Model**
* Key characteristics:
  * Company Form Name treated
  * Noise treatment: excluded some words, Google News, and other trusted sources

* Summary statistics:
  * Total companies analyzed: 78
  * Companies with disaster-related news: 38
  * Companies without disaster-related news: 40
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import json

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Disaster keywords with advanced pattern matching
disaster_keywords = [
    "extreme heat", "heatwave", "heat dome", "urban heat island",
    "extreme cold", "polar vortex", "frost", "cold snap", "blizzard",
    "flood", "riverine flood", "urban flood", "coastal flood", "flash flood",
    "ice jam flood", "glacial lake outburst flood", "dam break flood", "mudslide",
    "wildfire", "forest fire", "bushfire", "peat fire", "urban wildfire", "wildfire smoke",
    "drought", "agricultural drought", "meteorological drought", "hydrological drought",
    "water scarcity", "desertification",
    "storm", "hurricane", "typhoon", "cyclone", "tornado", "hailstorm",
    "lightning storm", "derecho", "storm surge", "dust storm", "windstorm",
    "marine heatwave", "coral bleaching", "sea level rise", "coastal erosion",
    "permafrost thaw", "avalanche", "sea ice loss",
    "pest outbreaks", "disease outbreaks", "vector-borne diseases", "waterborne diseases",
    "fire-flood", "storm-fire", "drought-induced wildfire", "heatwave-power outage",
    "climate migration", "food insecurity", "economic losses"
]
disaster_pattern = re.compile("|".join(disaster_keywords), re.IGNORECASE)

# Common company suffixes for cleaning
common_suffixes = [
    " Inc", " Corp", " plc", " Co", " Ltd", " LLC", " Group", " Holdings", " Intl",
    " Incorporated", " Company", " Limited", " Enterprises", " Technologies", " Solutions"
]

# Full list of companies and cleaned versions
companies = {
    company: re.sub(f"({'|'.join(common_suffixes)})[.,]?$", "", company, flags=re.IGNORECASE).strip()
    for company in [
        "GE Aerospace", "Caterpillar Inc", "RTX Corporation", "Uber Technologies Inc.",
        "Union Pacific Corp", "Eaton Corp plc", "Honeywell Intl Inc", "Lockheed Martin",
        "Automatic Data Processing", "Deere & Co", "United Parcel Service Inc B", "Boeing Co",
        "Trane Technologies plc", "GE Vernova Inc", "Parker-Hannifin Corp", "General Dynamics",
        "TransDigm Group", "Waste Management Inc", "Northrop Grumman Corp", "Cintas Corp",
        "Illinois Tool Works Inc", "3M Co", "CSX Corporation", "Emerson Electric Co",
        "Carrier Global Corp.", "FedEx Corp", "Norfolk Southern Corp", "PACCAR Inc",
        "United Rentals Inc", "Johnson Controls International plc", "W.W. Grainger Inc",
        "L3Harris Technologies Inc", "Quanta Services Inc", "Copart Inc", "Cummins Inc",
        "Paychex Inc", "Fastenal Co", "Howmet Aerospace Inc.", "Republic Services Inc",
        "Otis Worldwide Corp", "Ingersoll Rand Inc.", "AMETEK Inc", "Verisk Analytics Inc",
        "Old Dominion Freight Line Inc", "Delta Air Lines", "Equifax Inc", "Wabtec",
        "Axon Enterprise Inc", "Xylem Inc", "Rockwell Automation Inc", "Fortive Corp",
        "Veralto Corp", "Dover Corp", "Broadridge Financial Solutions Inc.",
        "United Airlines Holding, Inc", "Hubbell Inc", "Leidos Holdings Inc", "Builders FirstSource",
        "Masco Corp", "Southwest Airlines Co", "Jacobs Solutions Inc.", "Snap On Inc",
        "Expeditors Intl of WA Inc", "Pentair PLC", "Stanley Black & Decker", "Textron Inc",
        "IDEX Corp", "J.B. Hunt Transport Services", "Nordson Corp", "Rollins Inc",
        "CH Robinson Worldwide Inc", "Allegion plc", "Dayforce, Inc.", "Huntington Ingalls Industries Inc.",
        "Generac Holdings Inc", "A.O. Smith Corp", "Paycom Software Inc", "Amentum Holdings Inc."
    ]
}

def scrape_company_news(company, cleaned_name):
    logging.info(f"Scraping news for: {company}")
    results = []

    query = f"{cleaned_name} climate disaster OR {cleaned_name} extreme weather OR {cleaned_name} climate impact"
    rss_url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"

    trusted_sources = [
        "BBC.com", "The Washington Post", "The Guardian", "Reuters",
        "The New York Times", "Bloomberg", "The Associated Press", "CNN",
        "Al Jazeera", "NPR", "Financial Times", "The Economist",
        "Nature.com", "National Geographic", "Scientific American",
        "InsideClimate News", "Carbon Brief", "Climate Central",
        "South China Morning Post", "The Straits Times"
    ]
    exclude_keywords = ["stormed through", "perfect storm", "business model", "innovation"]

    try:
        response = requests.get(rss_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "xml")

        items = soup.find_all("item")
        seen_titles = set()

        for item in items:
            title = item.title.text.strip()
            description = item.description.text.strip() if item.description else "No description available"
            source = item.source.text.strip() if item.source else "Unknown"
            pub_date = item.pubDate.text if item.pubDate else None

            # Check if article is from a trusted source
            is_trusted = source in trusted_sources

            # Remove duplicates
            if title.lower() in seen_titles:
                continue
            seen_titles.add(title.lower())

            # Exclude irrelevant articles
            if any(keyword in title.lower() for keyword in exclude_keywords):
                continue

            # Check publication date (2020–2023)
            if pub_date:
                pub_date_obj = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %Z")
                if not (datetime(2020, 1, 1) <= pub_date_obj <= datetime(2023, 12, 31)):
                    continue
            else:
                continue

            # Check if disaster keywords are in title or description
            if not (disaster_pattern.search(title) or disaster_pattern.search(description)):
                continue

            # Add filtered results
            results.append({
                "title": title,
                "description": description,
                "source": source,
                "is_trusted": is_trusted,
                "date": pub_date_obj.strftime("%Y-%m-%d")
            })

    except Exception as e:
        logging.error(f"Error scraping news for {company}: {e}")

    return {"company": company, "headlines": results} if results else None

def scrape_disaster_related_news():
    all_results = []
    companies_with_news = []
    companies_without_news = []

    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(scrape_company_news, company, cleaned_name): company for company, cleaned_name in companies.items()}

        for future in as_completed(futures):
            result = future.result()
            company = futures[future]
            if result:
                all_results.append(result)
                companies_with_news.append(company)
                # Print results for immediate visibility
                print(f"Company: {result['company']}")
                for headline in result["headlines"]:
                    trusted = "Trusted Source" if headline["is_trusted"] else "Untrusted Source"
                    print(f"  - {headline['title']} ({trusted}, Published: {headline['date']})")
            else:
                companies_without_news.append(company)

    # Save results to JSON for further analysis
    with open("disaster_news.json", "w") as f:
        json.dump(all_results, f, indent=4)

    # Display summary
    print("\nSummary:")
    print(f"Total companies analyzed: {len(companies)}")
    print(f"Companies with disaster-related news: {len(companies_with_news)}")
    print(f"Companies without disaster-related news: {len(companies_without_news)}")

    # Optionally save the summary
    summary = {
        "total_companies": len(companies),
        "companies_with_news": companies_with_news,
        "companies_without_news": companies_without_news
    }
    with open("summary.json", "w") as f:
        json.dump(summary, f, indent=4)

if __name__ == "__main__":
    scrape_disaster_related_news()

"""**Last, Last**

* Summary Statistics:
* Total Companies with Climate Related News: 38 out of 78
* Total States with Climate Related News: 46 out of 50
* Total Articles with Climate Related News Found: 425
  * Articles Per Year:
    * 2020: 15 articles
    * 2021: 86 articles
    * 2022: 88 articles
    * 2023: 236 articles

**FINALLLLLLLLL**

**Test**
"""

!pip install lxml[html_clean]
!pip install requests-html

!pip install requests-html

import requests
from bs4 import BeautifulSoup
from requests_html import HTMLSession
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import json

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Disaster keywords with advanced pattern matching
disaster_keywords = [
    "extreme heat", "heatwave", "heat dome", "urban heat island",
    "extreme cold", "polar vortex", "frost", "cold snap", "blizzard",
    "flood", "riverine flood", "urban flood", "coastal flood", "flash flood",
    "ice jam flood", "glacial lake outburst flood", "dam break flood", "mudslide",
    "wildfire", "forest fire", "bushfire", "peat fire", "urban wildfire", "wildfire smoke",
    "drought", "agricultural drought", "meteorological drought", "hydrological drought",
    "water scarcity", "desertification",
    "storm", "hurricane", "typhoon", "cyclone", "tornado", "hailstorm",
    "lightning storm", "derecho", "storm surge", "dust storm", "windstorm",
    "marine heatwave", "coral bleaching", "sea level rise", "coastal erosion",
    "permafrost thaw", "avalanche", "sea ice loss",
    "pest outbreaks", "disease outbreaks", "vector-borne diseases", "waterborne diseases",
    "fire-flood", "storm-fire", "drought-induced wildfire", "heatwave-power outage",
    "climate migration", "food insecurity", "economic losses"
]
disaster_pattern = re.compile("|".join(disaster_keywords), re.IGNORECASE)

# Common company suffixes for cleaning
common_suffixes = [
    " Inc", " Corp", " plc", " Co", " Ltd", " LLC", " Group", " Holdings", " Intl",
    " Incorporated", " Company", " Limited", " Enterprises", " Technologies", " Solutions"
]

# Full list of companies and cleaned versions
companies = {
    company: re.sub(f"({'|'.join(common_suffixes)})[.,]?$", "", company, flags=re.IGNORECASE).strip()
    for company in [
        "GE Aerospace", "Caterpillar Inc", "RTX Corporation", "Uber Technologies Inc.",
        "Union Pacific Corp", "Eaton Corp plc", "Honeywell Intl Inc", "Lockheed Martin",
        "Automatic Data Processing", "Deere & Co", "United Parcel Service Inc B", "Boeing Co",
        "Trane Technologies plc", "GE Vernova Inc", "Parker-Hannifin Corp", "General Dynamics",
        "TransDigm Group", "Waste Management Inc", "Northrop Grumman Corp", "Cintas Corp",
        "Illinois Tool Works Inc", "3M Co", "CSX Corporation", "Emerson Electric Co",
        "Carrier Global Corp.", "FedEx Corp", "Norfolk Southern Corp", "PACCAR Inc",
        "United Rentals Inc", "Johnson Controls International plc", "W.W. Grainger Inc",
        "L3Harris Technologies Inc", "Quanta Services Inc", "Copart Inc", "Cummins Inc",
        "Paychex Inc", "Fastenal Co", "Howmet Aerospace Inc.", "Republic Services Inc",
        "Otis Worldwide Corp", "Ingersoll Rand Inc.", "AMETEK Inc", "Verisk Analytics Inc",
        "Old Dominion Freight Line Inc", "Delta Air Lines", "Equifax Inc", "Wabtec",
        "Axon Enterprise Inc", "Xylem Inc", "Rockwell Automation Inc", "Fortive Corp",
        "Veralto Corp", "Dover Corp", "Broadridge Financial Solutions Inc.",
        "United Airlines Holding, Inc", "Hubbell Inc", "Leidos Holdings Inc", "Builders FirstSource",
        "Masco Corp", "Southwest Airlines Co", "Jacobs Solutions Inc.", "Snap On Inc",
        "Expeditors Intl of WA Inc", "Pentair PLC", "Stanley Black & Decker", "Textron Inc",
        "IDEX Corp", "J.B. Hunt Transport Services", "Nordson Corp", "Rollins Inc",
        "CH Robinson Worldwide Inc", "Allegion plc", "Dayforce, Inc.", "Huntington Ingalls Industries Inc.",
        "Generac Holdings Inc", "A.O. Smith Corp", "Paycom Software Inc", "Amentum Holdings Inc."
    ]
}

trusted_archives = {
    "BBC": "https://www.bbc.co.uk/archive",
    "The Washington Post": "https://www.washingtonpost.com/archive/",
    "The Guardian": "https://www.theguardian.com/archive",
    "Reuters": "https://www.reuters.com/archive/",
    "The New York Times": "https://www.nytimes.com/section/archives",
    "Bloomberg": "https://www.bloomberg.com/archive",
    "The Associated Press": "https://apnews.com/hub/ap-archive",
    "CNN": "https://www.cnn.com/archives",
    "Al Jazeera": "https://www.aljazeera.com/archive/",
    "NPR": "https://www.npr.org/sections/archives/",
    "Financial Times": "https://www.ft.com/archives",
    "The Economist": "https://www.economist.com/archive",
    "Nature": "https://www.nature.com/nature/articles",
    "National Geographic": "https://www.nationalgeographic.com/archive",
    "Scientific American": "https://www.scientificamerican.com/archive/",
    "InsideClimate News": "https://insideclimatenews.org/archive/",
    "Carbon Brief": "https://www.carbonbrief.org/archive/",
    "Climate Central": "https://www.climatecentral.org/archive",
    "The Straits Times": "https://www.straitstimes.com/archive"
}

exclude_keywords = ["stormed through", "perfect storm", "business model", "innovation"]

# Google News scraper function (integrating trusted source checking)
def scrape_google_news(company, cleaned_name, trusted_sources):
    logging.info(f"Scraping news for: {company}")
    results = []

    query = f"{cleaned_name} climate disaster OR {cleaned_name} extreme weather OR {cleaned_name} climate impact"
    rss_url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"

    try:
        response = requests.get(rss_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "xml")

        for item in soup.find_all("item"):
            title = item.title.text.strip()
            description = item.description.text.strip() if item.description else "No description available"
            source = item.source.text.strip() if item.source else "Unknown"
            pub_date = item.pubDate.text if item.pubDate else None

            if any(source in trusted_sources for source in trusted_archives.keys()):
                results.append({
                    "title": title,
                    "description": description,
                    "source": source,
                    "trusted": source in trusted_archives.keys(),
                    "date": pub_date
                })

    except Exception as e:
        logging.error(f"Error scraping news for {company}: {e}")

    return {"company": company, "headlines": results} if results else None

# Function to scrape trusted archives separately
def scrape_trusted_archive(source_name, url):
    logging.info(f"Scraping archive for: {source_name}")
    session = HTMLSession()

    try:
        r = session.get(url)
        r.html.render(sleep=1, scrolldown=3)

        articles = []
        for article in r.html.find('article, h2, h3'):
            try:
                title = article.text.strip()
                link = list(article.absolute_links)[0] if article.absolute_links else None
                if title and link:
                    articles.append({"title": title, "link": link})
            except Exception as e:
                logging.error(f"Error processing article on {source_name}: {e}")

        return {"source": source_name, "articles": articles}

    except Exception as e:
        logging.error(f"Error scraping archive for {source_name}: {e}")
        return {"source": source_name, "articles": []}

if __name__ == "__main__":
    # Scraping from Google News
    google_results = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(scrape_google_news, company, cleaned_name, trusted_archives.keys())
                   for company, cleaned_name in companies.items()]
        for future in as_completed(futures):
            google_results.append(future.result())

    # Scraping trusted archives directly
    archive_results = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(scrape_trusted_archive, source, url)
                   for source, url in trusted_archives.items()]
        for future in as_completed(futures):
            archive_results.append(future.result())

    # Save both results
    with open("google_news_results.json", "w") as f:
        json.dump(google_results, f, indent=4)

    with open("trusted_archives_results.json", "w") as f:
        json.dump(archive_results, f, indent=4)

    print("\nScraping completed. Results saved to google_news_results.json and trusted_archives_results.json")

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import re
import json
import csv

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Disaster keywords with advanced pattern matching
disaster_keywords = [
    "extreme heat", "heatwave", "heat dome", "urban heat island",
    "extreme cold", "polar vortex", "frost", "cold snap", "blizzard",
    "flood", "riverine flood", "urban flood", "coastal flood", "flash flood",
    "ice jam flood", "glacial lake outburst flood", "dam break flood", "mudslide",
    "wildfire", "forest fire", "bushfire", "peat fire", "urban wildfire", "wildfire smoke",
    "drought", "agricultural drought", "meteorological drought", "hydrological drought",
    "water scarcity", "desertification",
    "storm", "hurricane", "typhoon", "cyclone", "tornado", "hailstorm",
    "lightning storm", "derecho", "storm surge", "dust storm", "windstorm",
    "marine heatwave", "coral bleaching", "sea level rise", "coastal erosion",
    "permafrost thaw", "avalanche", "sea ice loss",
    "pest outbreaks", "disease outbreaks", "vector-borne diseases", "waterborne diseases",
    "fire-flood", "storm-fire", "drought-induced wildfire", "heatwave-power outage",
    "climate migration", "food insecurity", "economic losses"
]
disaster_pattern = re.compile("|".join(disaster_keywords), re.IGNORECASE)

# State names
states = [
    "Florida", "New York"
]

# Common company suffixes for cleaning
common_suffixes = [
    " Inc", " Corp", " plc", " Co", " Ltd", " LLC", " Group", " Holdings", " Intl",
    " Incorporated", " Company", " Limited", " Enterprises", " Technologies", " Solutions"
]

# Full list of companies and cleaned versions
companies = {
    company: re.sub(f"({'|'.join(common_suffixes)})[.,]?$", "", company, flags=re.IGNORECASE).strip()
    for company in [
        "GE Aerospace", "Caterpillar Inc", "RTX Corporation", "Uber Technologies Inc.",
        "Union Pacific Corp", "Eaton Corp plc", "Honeywell Intl Inc", "Lockheed Martin",
        "Automatic Data Processing", "Deere & Co", "United Parcel Service Inc B", "Boeing Co",
        "Trane Technologies plc", "GE Vernova Inc", "Parker-Hannifin Corp", "General Dynamics",
        "TransDigm Group", "Waste Management Inc", "Northrop Grumman Corp", "Cintas Corp",
        "Illinois Tool Works Inc", "3M Co", "CSX Corporation", "Emerson Electric Co",
        "Carrier Global Corp.", "FedEx Corp", "Norfolk Southern Corp", "PACCAR Inc",
        "United Rentals Inc", "Johnson Controls International plc", "W.W. Grainger Inc",
        "L3Harris Technologies Inc", "Quanta Services Inc", "Copart Inc", "Cummins Inc",
        "Paychex Inc", "Fastenal Co", "Howmet Aerospace Inc.", "Republic Services Inc",
        "Otis Worldwide Corp", "Ingersoll Rand Inc.", "AMETEK Inc", "Verisk Analytics Inc",
        "Old Dominion Freight Line Inc", "Delta Air Lines", "Equifax Inc", "Wabtec",
        "Axon Enterprise Inc", "Xylem Inc", "Rockwell Automation Inc", "Fortive Corp",
        "Veralto Corp", "Dover Corp", "Broadridge Financial Solutions Inc.",
        "United Airlines Holding, Inc", "Hubbell Inc", "Leidos Holdings Inc", "Builders FirstSource",
        "Masco Corp", "Southwest Airlines Co", "Jacobs Solutions Inc.", "Snap On Inc",
        "Expeditors Intl of WA Inc", "Pentair PLC", "Stanley Black & Decker", "Textron Inc",
        "IDEX Corp", "J.B. Hunt Transport Services", "Nordson Corp", "Rollins Inc",
        "CH Robinson Worldwide Inc", "Allegion plc", "Dayforce, Inc.", "Huntington Ingalls Industries Inc.",
        "Generac Holdings Inc", "A.O. Smith Corp", "Paycom Software Inc", "Amentum Holdings Inc."
    ]
}

def scrape_google_news(query):
    logging.info(f"Scraping Google News for query: {query}")
    results = []
    rss_url = f"https://news.google.com/rss/search?q={query}&hl=en-US&gl=US&ceid=US:en"

    try:
        response = requests.get(rss_url)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "xml")

        items = soup.find_all("item")
        seen_titles = set()

        for item in items:
            title = item.title.text.strip()
            description = item.description.text.strip() if item.description else "No description available"
            source = item.source.text.strip() if item.source else "Unknown"
            pub_date = item.pubDate.text if item.pubDate else None

            # Remove duplicates
            if title.lower() in seen_titles:
                continue
            seen_titles.add(title.lower())

            # Check publication date (2020–2023)
            if pub_date:
                pub_date_obj = datetime.strptime(pub_date, "%a, %d %b %Y %H:%M:%S %Z")
                if not (datetime(2020, 1, 1) <= pub_date_obj <= datetime(2023, 12, 31)):
                    continue
            else:
                continue

            # Check if disaster keywords are in title or description
            if not (disaster_pattern.search(title) or disaster_pattern.search(description)):
                continue

            # Add filtered results
            results.append({
                "title": title,
                "description": description,
                "source": source,
                "is_trusted": source.lower() in [
                    "bbc.com", "the washington post", "the guardian", "reuters",
                    "the new york times", "bloomberg", "the associated press", "cnn",
                    "al jazeera", "npr", "financial times", "the economist",
                    "nature.com", "national geographic", "scientific american",
                    "insideclimate news", "carbon brief", "climate central",
                    "south china morning post", "the straits times"
                ],
                "date": pub_date_obj.strftime("%Y-%m-%d")
            })

    except Exception as e:
        logging.error(f"Error scraping Google News for query '{query}': {e}")

    return results

def scrape_disaster_related_news():
    all_results = {}

    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = {}

        # Company queries
        for company, cleaned_name in companies.items():
            query = f"{cleaned_name} climate disaster OR {cleaned_name} extreme weather OR {cleaned_name} climate impact"
            futures[executor.submit(scrape_google_news, query)] = company

        # State-level queries
        for state in states:
            query = f"{state} climate disaster OR {state} extreme weather OR {state} climate impact"
            futures[executor.submit(scrape_google_news, query)] = state

        for future in as_completed(futures):
            results = future.result()
            key = futures[future]

            if results:
                if key not in all_results:
                    all_results[key] = []
                all_results[key].extend(results)

    # Print results to console
    for key, results in all_results.items():
        print(f"\n{'Company' if key in companies else 'State'}: {key}")
        for result in results:
            trusted_status = "Trusted Source" if result["is_trusted"] else "Untrusted Source"
            print(f"  - {result['title']} - {result['source']} ({trusted_status}, Published: {result['date']})")

    # Summary statistics
    company_count = sum(1 for key in all_results if key in companies)
    state_count = sum(1 for key in all_results if key in states)
    total_articles = sum(len(results) for results in all_results.values())

    articles_per_year = {}
    csv_rows = []
    for key, results in all_results.items():
        for result in results:
            year = result['date'][:4]
            articles_per_year[year] = articles_per_year.get(year, 0) + 1

            csv_rows.append({
                "Entity": key,
                "Type": "Company" if key in companies else "State",
                "Title": result['title'],
                "Source": result['source'],
                "Trusted": "Yes" if result['is_trusted'] else "No",
                "Date": result['date']
            })

    print("\nSummary Statistics:")
    print(f"Total Companies with Climate Related News: {company_count} out of {len(companies)}")
    print(f"Total States with Climate Related News: {state_count} out of {len(states)}")
    print(f"Total Articles with Climate Related News Found: {total_articles}")
    print("Articles Per Year:")
    for year, count in sorted(articles_per_year.items()):
        print(f"  {year}: {count} articles")

    # Save results to JSON for further analysis
    with open("disaster_news.json", "w") as f:
        json.dump(all_results, f, indent=4)

    # Save results to CSV for QGIS
    csv_file = "disaster_news.csv"
    with open(csv_file, mode="w", newline="", encoding="utf-8") as file:
        writer = csv.DictWriter(file, fieldnames=["Entity", "Type", "Title", "Source", "Trusted", "Date"])
        writer.writeheader()
        writer.writerows(csv_rows)

    logging.info(f"Scraping completed. Total entities analyzed: {len(all_results)}")
    logging.info(f"CSV file saved as {csv_file}")

if __name__ == "__main__":
    scrape_disaster_related_news()

"""## Attempt to improve model

"""